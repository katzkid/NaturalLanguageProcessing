{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8515f57b",
   "metadata": {},
   "source": [
    "## Document classification notebook\n",
    "\n",
    "This notebook illustrates the lecture on document classification. We will go through the steps towards basic\n",
    "document classification systems with a bag of words representation and tf-idf weighting associaed with k-nn search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a308f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f5f3f1ef400>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f600bad9720>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f5f50a03d10>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f5f523bcf40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f5f3f172ec0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f5f50a02ea0>)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load a bunch of modules\n",
    "#\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "process = spacy.load('en_core_web_md')\n",
    "print(process.pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a020130",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data that we will use comes from a super popular toy dataset for opinion mining: IMDb. The dataset provides reviews from the IMDb website where half of the reviews express positive comments and half negative. The task is this geared towards detecting the _polarity_ of the opinion expressed.\n",
    "\n",
    "See https://ai.stanford.edu/~amaas/data/sentiment for details. You can also get the raw data from there but I'm providing you with an easier json version to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc257064",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./imdb-trn.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m     imdb_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of utterances =\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(imdb_data))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of + utterances =\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m imdb_data \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniforge3/envs/NLPenv/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLPenv/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load IMDb data\n",
    "#\n",
    "fn = 'imdb-trn.json'\n",
    "\n",
    "with open(fn, 'rt') as f:\n",
    "    imdb_data = json.load(f)\n",
    "    \n",
    "print('number of utterances =', len(imdb_data))\n",
    "print('number of + utterances =', len([x for x in imdb_data if x[0] == 'pos']))\n",
    "print('number of - utterances =', len([x for x in imdb_data if x[0] == 'neg']))\n",
    "print('data[0] =', imdb_data[0])\n",
    "print('data[-1] =', imdb_data[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5515674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances = 2000\n",
      "number of + utterances = 1000\n",
      "number of - utterances = 1000\n",
      "data[0] = ['pos', 'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
      "data[-1] = ['neg', 'Not that I dislike childrens movies, but this was a tearjerker with few redeeming qualities. M.J. Fox was the perfect voice for Stuart and the rest of the talent was wasted. Hugh Laurie can be amazingly funny, but is not given the chance in this movie. ItÂ´s sugar-coated sugar and would hardly appeal to anyone over 7 years of age. See Toy Story, Monsters Inc. or Shrek instead. 3/10']\n",
      "number of test utterances = 400\n",
      "number of + test utterances = 200\n",
      "number of - test utterances = 200\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Make a smaller dataset so we can go fast, splitting into train and test.  \n",
    "#\n",
    "\n",
    "ntrain_per_class = 1000\n",
    "ntest_per_class = 200\n",
    "\n",
    "imdb_data_small = imdb_data[:ntrain_per_class] + imdb_data[-ntrain_per_class:]\n",
    "nutterances = len(imdb_data_small)\n",
    "\n",
    "print('number of utterances =', nutterances)\n",
    "print('number of + utterances =', len([x for x in imdb_data_small if x[0] == 'pos']))\n",
    "print('number of - utterances =', len([x for x in imdb_data_small if x[0] == 'neg']))\n",
    "print('data[0] =', imdb_data_small[0])\n",
    "print('data[-1] =', imdb_data_small[-1])\n",
    "\n",
    "n1 = ntrain_per_class\n",
    "n2 = ntrain_per_class + ntest_per_class\n",
    "imdb_data_test = imdb_data[n1:n2] + imdb_data[-n2:-n1]\n",
    "print('number of test utterances =', len(imdb_data_test))\n",
    "print('number of + test utterances =', len([x for x in imdb_data_test if x[0] == 'pos']))\n",
    "print('number of - test utterances =', len([x for x in imdb_data_test if x[0] == 'neg']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9f846",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "To make things faster, we will pre-process the data (tokenization, POS tagging, lemmatization) with spaCy's English pipeline. If you have some time to waste, you can try to do that explictly with NLTK's tokenizer, POS tagger and lemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44eeb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'pos', 'token': ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.'], 'pos': ['IN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', '.', 'VB', 'DT', 'NN', 'WRB', 'NNP', 'NNP', 'VBZ', 'RB', 'JJ', '.', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'NN', '.', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.', 'VB', 'IN', 'NNP', '``', 'DT', 'NNP', \"''\", 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'NNP', '.'], 'lemma': ['for', 'a', 'movie', 'that', 'get', 'no', 'respect', 'there', 'sure', 'be', 'a', 'lot', 'of', 'memorable', 'quote', 'list', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'be', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'be', 'a', 'scene', 'stealer', '.', 'the', 'Moroni', 'character', 'be', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'Alan', '\"', 'the', 'Skipper', '\"', 'Hale', 'jr', '.', 'as', 'a', 'police', 'Sgt', '.']}\n",
      "{'label': 'neg', 'token': ['not', 'that', 'i', 'dislike', 'childrens', 'movies', ',', 'but', 'this', 'was', 'a', 'tearjerker', 'with', 'few', 'redeeming', 'qualities', '.', 'm.j.', 'fox', 'was', 'the', 'perfect', 'voice', 'for', 'stuart', 'and', 'the', 'rest', 'of', 'the', 'talent', 'was', 'wasted', '.', 'hugh', 'laurie', 'can', 'be', 'amazingly', 'funny', ',', 'but', 'is', 'not', 'given', 'the', 'chance', 'in', 'this', 'movie', '.', 'itÂ´s', 'sugar', '-', 'coated', 'sugar', 'and', 'would', 'hardly', 'appeal', 'to', 'anyone', 'over', '7', 'years', 'of', 'age', '.', 'see', 'toy', 'story', ',', 'monsters', 'inc.', 'or', 'shrek', 'instead', '.', '3/10'], 'pos': ['RB', 'IN', 'PRP', 'VBP', 'NNS', 'NNS', ',', 'CC', 'DT', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'VBG', 'NNS', '.', 'NNP', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', '.', 'NNP', 'NNP', 'MD', 'VB', 'RB', 'JJ', ',', 'CC', 'VBZ', 'RB', 'VBN', 'DT', 'NN', 'IN', 'DT', 'NN', '.', 'NNP', 'NN', 'HYPH', 'VBN', 'NN', 'CC', 'MD', 'RB', 'VB', 'IN', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', '.', 'VB', 'NNP', 'NNP', ',', 'NNPS', 'NNP', 'CC', 'NNP', 'RB', '.', 'CD'], 'lemma': ['not', 'that', 'I', 'dislike', 'children', 'movie', ',', 'but', 'this', 'be', 'a', 'tearjerker', 'with', 'few', 'redeem', 'quality', '.', 'M.J.', 'Fox', 'be', 'the', 'perfect', 'voice', 'for', 'Stuart', 'and', 'the', 'rest', 'of', 'the', 'talent', 'be', 'waste', '.', 'Hugh', 'Laurie', 'can', 'be', 'amazingly', 'funny', ',', 'but', 'be', 'not', 'give', 'the', 'chance', 'in', 'this', 'movie', '.', 'ItÂ´s', 'sugar', '-', 'coat', 'sugar', 'and', 'would', 'hardly', 'appeal', 'to', 'anyone', 'over', '7', 'year', 'of', 'age', '.', 'see', 'Toy', 'Story', ',', 'Monsters', 'Inc.', 'or', 'Shrek', 'instead', '.', '3/10']}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We will process the text with spaCy for tokenization, POS tagging and lemmatization. We will process each text\n",
    "# in the database and create a new representation of the database as a list where each entry is a dictionary with\n",
    "# label, tokens, POS Tags, lemmas.\n",
    "#\n",
    "# For efficiency reasons, we call spaCy's processing in a special way (with pipe() that enables parallelization)\n",
    "# and disable the elements of the pipeline we will not use (i.e, depency parsing and named entity recognition).\n",
    "# This results in a list of processed texts from which we can read the tokens, POS tags and lemmas as we did\n",
    "# for lecture 1.\n",
    "#\n",
    "\n",
    "database = imdb_data_small\n",
    "nutterances = len(database)\n",
    "\n",
    "raw_texts = [x[1] for x in database]\n",
    "processed_texts = list(process.pipe(raw_texts, disable=['parser', 'ner']))\n",
    "    \n",
    "# initialize output structure\n",
    "data = []\n",
    "\n",
    "\n",
    "for i in range(nutterances):\n",
    "    buf = {}\n",
    "    buf['label'] = database[i][0]\n",
    "    buf['token'] = [t.text.lower() for t in processed_texts[i]]\n",
    "    buf['pos'] = [t.tag_ for t in processed_texts[i]]\n",
    "    buf['lemma'] = [t.lemma_ for t in processed_texts[i]]    # already lowercased\n",
    "    \n",
    "    data.append(buf)\n",
    "\n",
    "print(data[0])\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864ccaa",
   "metadata": {},
   "source": [
    "### Choose vocabulary\n",
    "\n",
    "The _vocabulary_ is the list of terms that we will consider to represent the documents. We'll limit ourselves to simple terms (as opposed to complex terms such as 'can opener' or 'neural network') and simply select the most frequent terms in the corpus.\n",
    "\n",
    "We provide the function get_token_counts() to help  doing that. The function takes the database as input and returns a list of tokens therein (no filtering in initial version) with the number of times they appear. Output\n",
    "is a dictionary with token: count sorted in descending order w.r.t. the number of occurrences in the database \n",
    "\n",
    "Note that many toolkits for NLP provide a sort of equivalent function, e.g., \n",
    "   gensim.corpora.dictionary.Dictionary -- https://radimrehurek.com/gensim/corpora/dictionary.html#\n",
    "   tf.keras.preprocessing.text.Tokenizer -- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "But we'll do it with our own function to fully understand what's going on behind the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2234680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in dataset = 29331\n",
      "most frequent tokens:\n",
      "   the                   26264\n",
      "   ,                     21180\n",
      "   .                     19143\n",
      "   a                     13139\n",
      "   and                   13060\n",
      "   of                    11648\n",
      "   to                    11002\n",
      "   is                    8892\n",
      "   in                    7479\n",
      "   it                    7427\n",
      "   i                     6857\n",
      "   this                  5930\n",
      "   that                  5899\n",
      "   \"                     5135\n",
      "   's                    4961\n",
      "   -                     4438\n",
      "   was                   4162\n",
      "   /><br                 4104\n",
      "   as                    3668\n",
      "   for                   3512\n",
      "\n",
      "least frequent tokens:\n",
      "   castles               1\n",
      "   too)                  1\n",
      "   evelyn                1\n",
      "   blanc                 1\n",
      "   />zero/10             1\n",
      "   cigars                1\n",
      "   constitute            1\n",
      "   afleck                1\n",
      "   gigli.<br             1\n",
      "   pointlessly           1\n",
      "   parody.<br            1\n",
      "   repleat               1\n",
      "   puffed                1\n",
      "   jowls                 1\n",
      "   funny).<br            1\n",
      "   />throw               1\n",
      "   camora                1\n",
      "   capiche               1\n",
      "   overcoat              1\n",
      "   m.j.                  1\n"
     ]
    }
   ],
   "source": [
    "def get_token_counts(idata, use_lemma = False, reserved = ()):\n",
    "    '''\n",
    "    Create vocabulary from a bunch of (tokenized) texts. If use_lemma == True, use lemma rather than\n",
    "    tokens. The tuple 'reserved' can be provided to initialize the list of tokens with reserved\n",
    "    names (e.g., [PAD], [UNK], [START], [END])\n",
    "    \n",
    "    No filtering on the tokens is implemented.\n",
    "    \n",
    "    Returns:\n",
    "        - token to id mapping (dict)\n",
    "        - token count (dict)\n",
    "    '''\n",
    "\n",
    "    tokcnt = {x: 0 for x in reserved} \n",
    "    \n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    for sample in idata:\n",
    "        \n",
    "        utterance = sample[kw]\n",
    "\n",
    "        for i, token in enumerate(utterance):\n",
    "            #\n",
    "            # if we were to implement filters, e.g., on the POS tags, that's the place\n",
    "            # where it could/should be done.\n",
    "            #\n",
    "            # e.g., if sample['pos'][i][0] not in ('N', 'V', 'J'): continue\n",
    "            #\n",
    "            tokcnt[token] = 1 if token not in tokcnt else tokcnt[token] + 1\n",
    "\n",
    "    return dict(sorted(tokcnt.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "\n",
    "count = get_token_counts(data)\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(count))\n",
    "print('most frequent tokens:')\n",
    "for x in list(count.keys())[:20]:\n",
    "    print(f\"   {x:20}  {count[x]}\")\n",
    "print('\\nleast frequent tokens:')\n",
    "for x in list(count.keys())[-20:]:\n",
    "    print(f\"   {x:20}  {count[x]}\")\n",
    "\n",
    "#\n",
    "# We select the top 2,000 tokens as our vocabulary to construct bag of words representations of the\n",
    "# documents. The vocabulary will be dictionary mapping string to ids, the latter ranging from 0 to 1,999\n",
    "#\n",
    "nterms = 2000\n",
    "vocab = {x: i for i,x in enumerate(list(count.keys())[:nterms])}\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Comment the most and least frequent tokens and think about how you could get\n",
    "# a list of tokens of interest other than by selecting the most frequent ones.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66605c",
   "metadata": {},
   "source": [
    "### Compute idf weighting\n",
    " \n",
    "Inverse document frequency (idf) is proportional to the number of samples in the database that contain a token. We want to compute that for every token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8646b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token = i noccs = 1592 idf = 0.09908693226233099\n",
      "token = pretentious noccs = 22 idf = 1.9586073148417749\n"
     ]
    }
   ],
   "source": [
    "def get_num_doc(idata, token, use_lemma = False):\n",
    "    '''\n",
    "    Returns the number of samples in data that contains the specified token. If use_lemma is True,\n",
    "    search in the lemma field, else in the token field.\n",
    "    \n",
    "    Note: if a filter is implemented in get_token_counts(), this function should be adapted accordingly\n",
    "    because the wordform can become ambiguous.\n",
    "    '''\n",
    "    \n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    return len([x for x in idata if token in x[kw]])\n",
    "\n",
    "\n",
    "#\n",
    "# Make an idf dictionary where we store the idf for each token\n",
    "#\n",
    "\n",
    "idf = np.zeros(len(vocab), dtype=np.float64)\n",
    "for term, idx in vocab.items():\n",
    "    idf[idx] = np.log10(nutterances / get_num_doc(data, term))\n",
    "\n",
    "#\n",
    "# Pretty print for control\n",
    "#\n",
    "w = list(vocab.keys())[10]\n",
    "print('token =', w, 'noccs =', get_num_doc(data, w), 'idf =', idf[10])\n",
    "\n",
    "w = list(vocab.keys())[-10]\n",
    "print('token =', w, 'noccs =', get_num_doc(data, w), 'idf =', idf[-10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ca5df",
   "metadata": {},
   "source": [
    "### Convert utterances to tf-idf vectors\n",
    "\n",
    "We first provide a function get_num_occ() that returns the number of occurrences of a given token in an utterance. Here again, this is to be adapted if you're vocabulary construction implies filtering on POS.\n",
    "\n",
    "We then use this function within a function tf_idf_vectorize() that converts a document into a tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c895e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
      "  word the       : idx=0  noccs=2  idf=0.003926345514724656\n",
      "  word a         : idx=3  noccs=5  idf=0.015247721884586453\n",
      "  word skipper   : idx=-1  noccs=1  idf=\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_num_occ(utterance, token, use_lemma = False):\n",
    "    '''\n",
    "    Return number of occurrences of a token in an utterance. \n",
    "    '''\n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    filtered = list(filter(lambda x: x == token, utterance[kw]))\n",
    "    \n",
    "    return len(filtered)\n",
    "\n",
    "print(data[0]['token'])\n",
    "for w in ('the', 'a', 'skipper'):\n",
    "    idx = vocab[w] if w in vocab else -1\n",
    "    print('  word {:10}: idx={}  noccs={}  idf={}'.format(w, idx, get_num_occ(data[0], w), idf[idx] if idx != -1 else ''))\n",
    "\n",
    "# X = np.empty((nutterances, ntokens), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3593a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
      "[0.00016026 0.         0.00055744 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tf_idf_vectorize(utt, voc, idf, use_lemma = False):\n",
    "    '''\n",
    "    Convert utterance utt to tf-idf vector with vocabulary in voc.\n",
    "    \n",
    "    Note that this is by far not an efficient implementation but you can at least follow \n",
    "    the different steps.\n",
    "    '''\n",
    "    \n",
    "    vec = np.zeros(len(voc), dtype=np.float64) \n",
    "    \n",
    "    # get number of occurrences of each term -- remember the vocabulary is a mapping from string to index/dimension\n",
    "    for term, idx in voc.items():\n",
    "        vec[idx] = get_num_occ(utt, term, use_lemma)\n",
    "    \n",
    "    # normalize number of occurrences to get a term frequency\n",
    "    vec = vec / sum(vec)\n",
    "    \n",
    "    # multiply tf by idf\n",
    "\n",
    "    return np.multiply(vec, idf)\n",
    "\n",
    "print(data[0]['token'])\n",
    "print(tf_idf_vectorize(data[0], vocab, idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "051b2ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:49<00:00, 40.01it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((nutterances, nterms), dtype=np.float64)\n",
    "for i in tqdm(range(nutterances)):\n",
    "    X[i,:] = tf_idf_vectorize(data[i], vocab, idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e85a18",
   "metadata": {},
   "source": [
    "### Train k-nn index to be able to retrieve k-nearest neighbors easily and test on test data\n",
    "\n",
    "We use the training data in X to train a k-nn search with cosine distance, prepare test data and run classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94812de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For utterance 0, the closest points are at indices: [   0  884 1307 1429  232]\n",
      "For utterance 0, the closest points are at distance: [0.         0.81670644 0.81773102 0.81989237 0.8217136 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine').fit(X)\n",
    "\n",
    "#\n",
    "# What if we search for the k-nearest neighbors of each of the samples in the dataset?\n",
    "#\n",
    "dist, idx = knn.kneighbors(X)\n",
    "print('For utterance 0, the closest points are at indices:', idx[0])\n",
    "print('For utterance 0, the closest points are at distance:', dist[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d599ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 400/400 [00:10<00:00, 38.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's prepare test data as we did for train data\n",
    "#\n",
    "\n",
    "# Step 1. tokenize, POS tag and lemmatize test data\n",
    "database = imdb_data_test\n",
    "ntests = len(database)\n",
    "\n",
    "test_raw_texts = [x[1] for x in database]\n",
    "test_processed_texts = list(process.pipe(test_raw_texts, disable=['parser', 'ner']))\n",
    "test_data = []\n",
    "\n",
    "for i in range(ntests):\n",
    "    buf = {}\n",
    "    buf['label'] = database[i][0]\n",
    "    buf['token'] = [t.text.lower() for t in test_processed_texts[i]]\n",
    "    buf['pos'] = [t.tag_ for t in test_processed_texts[i]]\n",
    "    buf['lemma'] = [t.lemma_ for t in test_processed_texts[i]]    # already lowercased\n",
    "    \n",
    "    test_data.append(buf)\n",
    "\n",
    "# Step 2. Vectorize test data\n",
    "Y = np.zeros((ntests, nterms), dtype=np.float64)\n",
    "for i in tqdm(range(ntests)):\n",
    "    Y[i,:] = tf_idf_vectorize(test_data[i], vocab, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14b1bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test utterance 0, the closest points are at indices: [ 708  604 1431  275 1571]\n",
      "For test utterance 0, the closest points are at distance: [0.74777752 0.76990411 0.78718233 0.79550038 0.81435515]\n",
      "Class for test utterance 0: pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 400/400 [00:00<00:00, 155806.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% + utterances correct = 70.50\n",
      "% - utterances correct = 62.50\n",
      "% utterances correct = 66.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_class(neighbors, th):\n",
    "    '''\n",
    "    Return class given the neighbors, assuming all neighbor indices below th are positive ones, \n",
    "    the remaining ones being negative.\n",
    "    '''\n",
    "    \n",
    "    npos = len(list(filter(lambda i: i < th, neighbors))) # count the number of positive neighbors\n",
    "    nneg = len(neighbors) - npos\n",
    "\n",
    "    return 'pos' if npos > nneg else 'neg'\n",
    " \n",
    "    \n",
    "dist, idx = knn.kneighbors(Y)\n",
    "\n",
    "print('For test utterance 0, the closest points are at indices:', idx[0])\n",
    "print('For test utterance 0, the closest points are at distance:', dist[0])\n",
    "print('Class for test utterance 0:', get_class(idx[0], ntrain_per_class))    \n",
    "\n",
    "\n",
    "nok = [0, 0]\n",
    "for i in tqdm(range(ntests)):\n",
    "    c = get_class(idx[i], ntrain_per_class)\n",
    "    if i < ntest_per_class and c == 'pos':\n",
    "        nok[0] += 1\n",
    "    elif i >= ntest_per_class and c == 'neg':\n",
    "        nok[1] += 1\n",
    "\n",
    "print('% + utterances correct = {:.2f}'.format(100 * nok[0] / ntest_per_class))\n",
    "print('% - utterances correct = {:.2f}'.format(100 * nok[1] / ntest_per_class))\n",
    "print('% utterances correct = {:.2f}'.format(100 * (nok[0] + nok[1]) / (2 * ntest_per_class)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d5ef5",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "We've been through the whole process: \n",
    "  1. utterance processing: tokenization, POS tagging, lemmmatization\n",
    "  2. term selection: in this example, simply the most frequent tokens\n",
    "  3. utterance vectorization: tf-idf weighting of the index term\n",
    "  4. train k-nn index: train index structure for fast k-nn search\n",
    "  5. k-nn classification: classify test data with k-nn classifier\n",
    "\n",
    "Now it's your turn to improve things via better term selection. Up to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
