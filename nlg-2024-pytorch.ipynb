{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c2bb47",
   "metadata": {},
   "source": [
    "# RNN language modeling and generation\n",
    "\n",
    "Building a RNN language model from IMDb data comments. The notebook walks you through the construction of a language model based on a LSTM recurrent neural network, following the usual steps: data preparation, vocabulary selection, model definition and training. Once trained, you're invited to use the language model as a text generator: this part is not implemented (yet) but you're provided with a toy generation loop with LSTM networks that should easily be adapated. Note that training LSTMs might take some time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1895dc",
   "metadata": {},
   "source": [
    "## Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24618e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load a bunch of modules\n",
    "#\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f647668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load IMDb data and process a small number of samples (positive here)\n",
    "#\n",
    "\n",
    "fn = './imdb-trn.json'\n",
    "\n",
    "with open(fn, 'rt') as f:\n",
    "    imdb_data = json.load(f)\n",
    "    \n",
    "\n",
    "data = imdb_data[:2000] + imdb_data[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5721665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21305 ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.']\n",
      "22673 ['i', 'read', 'comments', 'about', 'this', 'being', 'the', 'best', 'chinese', 'movie', 'ever', '.']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# basic data cleansing and preparation to yield two list of utterances: \n",
    "# texts[0] and texts[1] contain sentences from positive and negative comments respectively.\n",
    "#\n",
    "\n",
    "def clean_utterance(buf):\n",
    "    '''\n",
    "    Clean the list of tokens.\n",
    "    '''\n",
    "    ignore = (\"``\", \"''\", \"(\", \")\", '<', 'br', '/', '>', '--', '*', '-')\n",
    "    \n",
    "    return [x.lower() for x in buf if x.lower() not in ignore]\n",
    "\n",
    "\n",
    "texts = [[], []]\n",
    "\n",
    "for post in [x[1] for x in data if x[0] == 'pos']:\n",
    "    for utterance in sent_tokenize(post):\n",
    "        texts[0].append(clean_utterance(word_tokenize(utterance)))\n",
    "\n",
    "for post in [x[1] for x in data if x[0] == 'neg']:\n",
    "    for utterance in sent_tokenize(post):\n",
    "        texts[1].append(clean_utterance(word_tokenize(utterance)))\n",
    "\n",
    "print(len(texts[0]), texts[0][0])\n",
    "print(len(texts[1]), texts[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab693be",
   "metadata": {},
   "source": [
    "## Define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e45426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in dataset = 30536\n",
      "Number of tokens appearing more than 5 times = 7253\n",
      "['PAD', 'BOS', 'EOS', 'UNK', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'that', 'this', \"'s\", 'as', 'with']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# The ususal steps to define the vocabulary, here limiting ourselves to tokens that appear \n",
    "# at least 5 times in the positive utterances (we will build a model for positive comments only).\n",
    "#\n",
    "\n",
    "counter = Counter()\n",
    "   \n",
    "for item in texts[0]:\n",
    "    counter.update(item)\n",
    "\n",
    "counter = dict(sorted(counter.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(counter))\n",
    "# print('\\nleast frequent tokens:')\n",
    "# for x in list(counter.keys())[-20:]:\n",
    "#    print(f\"   {x:18}  {counter[x]}\")\n",
    "\n",
    "    \n",
    "MINOCC = 5 # keep only tokens that appear at least MINOCC times\n",
    "\n",
    "#\n",
    "# Special tokens in the vocabulary\n",
    "#    PAD : padding sequences to the same length\n",
    "#    BOS : begining of sentence\n",
    "#    EOS : end of sentence\n",
    "#    UNK : out-of-vocabulary (OOV) token\n",
    "#\n",
    "vocab = {'PAD': 0, 'BOS': 1, 'EOS': 2, 'UNK': 3}\n",
    "offset = len(vocab)\n",
    "vocab.update({x: i+offset for i, x in enumerate(list(counter.keys())) if counter[x] >= MINOCC})\n",
    "\n",
    "print(f'Number of tokens appearing more than {MINOCC} times =', len(vocab) - 4)\n",
    "print(list(vocab.keys())[:20])\n",
    "\n",
    "# build inverse mapping\n",
    "id2str = list(vocab.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079e06e",
   "metadata": {},
   "source": [
    "## Encode dataset, split into folds and convert to pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b46425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 20, 8, 24, 15, 239, 92, 1179, 52, 277, 28, 8, 177, 9, 612, 2825, 4931, 20, 16, 1007, 6], [1, 857, 8, 24, 121, 1050, 3, 11, 183, 201, 38], [1, 3714, 3, 11, 8, 136, 3, 6], [1, 4, 3, 112, 11, 39, 1584, 2527, 6], [1, 129, 20, 939, 4, 3, 5534, 1802, 18, 8, 480, 3, 6], [1, 1500, 251, 24, 1180, 19, 587, 1381, 23, 2409, 36, 3, 2205, 279, 9, 259, 17, 3, 1273, 18, 8, 219, 23, 551, 3, 2826, 19, 8, 3, 1413, 37, 11, 3, 10, 1940, 20, 40, 3200, 3, 36, 3, 4, 3, 10, 834, 38], [1, 4, 157, 19, 2205, 3, 28, 47, 73, 2009, 5, 4, 1103, 193, 11, 266, 5, 3, 3, 11, 772, 18, 3, 17, 4932, 5, 23, 50, 2205, 923, 86, 8, 940, 3715, 3716, 3, 3, 36, 8, 1687, 4933, 27, 4, 387, 1638, 5, 195, 74, 431, 4460, 6], [1, 4, 4461, 5, 481, 8, 3, 2664, 3, 3, 7, 3, 398, 4934, 2317, 267, 6248, 3, 5, 28, 8, 6249, 177, 5, 7, 3, 3, 11, 61, 217, 18, 8, 3, 480, 1027, 6], [1, 4, 24, 11, 808, 8, 3, 9, 4462, 17, 809, 7, 4, 3, 23, 53, 8, 2010, 38], [1, 402, 27, 4, 3, 36, 4038, 3, 5, 4, 6250, 11, 3, 2528, 5, 406, 9, 5535, 591, 148, 73, 36, 172, 410, 1453, 5, 37, 3, 8, 3, 3, 269, 19, 3201, 6]]\n",
      "BOS for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Encode all utterances given the vocabulary mapping\n",
    "#\n",
    "\n",
    "def encode_utterance(x: list[str]) -> list[int]:\n",
    "    '''\n",
    "    Encode an utterance according to the mapping provided by vocab. \n",
    "    '''\n",
    "    global vocab\n",
    "    \n",
    "    unk_id = vocab['UNK']\n",
    "    \n",
    "    return [vocab['BOS']] + [vocab.get(token, unk_id) for token in x]\n",
    "\n",
    "\n",
    "def decode_utterance(_ids: list[int]) -> str:\n",
    "    '''\n",
    "    Returns the string corresponding to the list of ids\n",
    "    '''\n",
    "    \n",
    "    global id2str\n",
    "    \n",
    "    return ' '.join([id2str[x] for x in _ids])\n",
    "\n",
    "\n",
    "#\n",
    "# Apply encode_utterance() on all positive texts\n",
    "#\n",
    "encodings = list(map(encode_utterance, texts[0]))\n",
    "\n",
    "print(encodings[:10])\n",
    "print(decode_utterance(encodings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65126243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens per utterance      min=1  max=622  mean=25.9  median=21  sdev=20.2\n",
      "Number of utterances with no OOV    7723 out of 21305\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Basic statistics on the entire dataset before we split\n",
    "#\n",
    "\n",
    "ntokens = [len(x) for x in encodings]\n",
    "\n",
    "m = statistics.mean(ntokens)\n",
    "m2 = statistics.median(ntokens)\n",
    "sdev = statistics.stdev(ntokens)\n",
    "\n",
    "print('{:35s} min={}  max={}  mean={:.1f}  median={}  sdev={:.1f}'.format('Number of tokens per utterance', min(ntokens), max(ntokens), m, m2, sdev))\n",
    "\n",
    "n_no_unk = len([x for x in encodings if vocab['UNK'] not in x])\n",
    "print('Number of utterances with no OOV   ', n_no_unk, 'out of {}'.format(len(encodings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61cfa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14913 3196 3196\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Make splits\n",
    "#\n",
    "\n",
    "fold = dict()\n",
    "\n",
    "fold['train'], buf = train_test_split(encodings, test_size=0.3, random_state=42)\n",
    "fold['valid'], fold['test'] = train_test_split(buf, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(fold['train']), len(fold['valid']), len(fold['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50ab06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create label sequences for a batch of token sequences\n",
    "#\n",
    "def make_labels(_data: list[list[int]]) -> list[list[int]]:\n",
    "    '''\n",
    "    Shift left input sequence and complete with the EOS token.\n",
    "    '''\n",
    "    \n",
    "    global vocab\n",
    "    \n",
    "    eos_id = vocab['EOS']\n",
    "\n",
    "    return [x[1:] + [eos_id] for x in _data]\n",
    "\n",
    "#\n",
    "# Pads a batch of sequences\n",
    "#\n",
    "def pad_sequences(_data: list[list[int]], maxlen: int = 0) -> list[list[int]]:\n",
    "    '''\n",
    "    Pad all input utterances up to maxlen, truncating if need be. If maxlen is null, will\n",
    "    be set to the length of the longest sequence\n",
    "    \n",
    "    Returns a list with padded ids. \n",
    "    '''\n",
    "    \n",
    "    global vocab\n",
    "    \n",
    "    pad_id = vocab['PAD']\n",
    "\n",
    "    if maxlen <= 0:\n",
    "        maxlen = max((len(x) for x in _data))\n",
    "    \n",
    "    return [(x + [pad_id] * (maxlen - len(x)))[:maxlen] for x in _data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01661981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = [[1, 23, 806, 4, 4369, 239, 2842, 1695, 12, 1051, 3179, 45, 18, 30, 10, 3, 5, 7, 53, 108, 33, 97, 39, 2714, 112, 3, 5, 199, 3, 60, 29, 3457, 86, 4, 5356, 16, 24, 11, 3, 5, 113, 381, 29, 252, 740, 76, 3, 3, 3, 17, 221, 797, 11, 293, 5890, 5, 18, 48, 576, 5653, 40, 171, 18, 4, 2912, 5485, 37, 11, 257, 4, 83, 3, 452, 27, 783, 12, 287, 10, 629, 19, 15, 114, 3, 135, 8, 335, 713, 9, 72, 5, 7, 60, 29, 191, 41, 13, 2842, 5, 83, 9, 69, 28, 9, 40, 169, 489, 6], [1, 20, 31, 197, 5, 13, 44, 39, 989, 154, 109, 2060, 3, 7, 1716, 1952, 5, 20, 585, 6]] /2/\n",
      "\n",
      "labels =  [[23, 806, 4, 4369, 239, 2842, 1695, 12, 1051, 3179, 45, 18, 30, 10, 3, 5, 7, 53, 108, 33, 97, 39, 2714, 112, 3, 5, 199, 3, 60, 29, 3457, 86, 4, 5356, 16, 24, 11, 3, 5, 113, 381, 29, 252, 740, 76, 3, 3, 3, 17, 221, 797, 11, 293, 5890, 5, 18, 48, 576, 5653, 40, 171, 18, 4, 2912, 5485, 37, 11, 257, 4, 83, 3, 452, 27, 783, 12, 287, 10, 629, 19, 15, 114, 3, 135, 8, 335, 713, 9, 72, 5, 7, 60, 29, 191, 41, 13, 2842, 5, 83, 9, 69, 28, 9, 40, 169, 489, 6, 2], [20, 31, 197, 5, 13, 44, 39, 989, 154, 109, 2060, 3, 7, 1716, 1952, 5, 20, 585, 6, 2]] /2/\n",
      "\n",
      "\n",
      "[[1, 23, 806, 4, 4369, 239, 2842, 1695, 12, 1051, 3179, 45, 18, 30, 10, 3, 5, 7, 53, 108, 33, 97, 39, 2714, 112, 3, 5, 199, 3, 60, 29, 3457, 86, 4, 5356, 16, 24, 11, 3, 5, 113, 381, 29, 252, 740, 76, 3, 3, 3, 17], [1, 20, 31, 197, 5, 13, 44, 39, 989, 154, 109, 2060, 3, 7, 1716, 1952, 5, 20, 585, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "\n",
      "[[23, 806, 4, 4369, 239, 2842, 1695, 12, 1051, 3179, 45, 18, 30, 10, 3, 5, 7, 53, 108, 33, 97, 39, 2714, 112, 3, 5, 199, 3, 60, 29, 3457, 86, 4, 5356, 16, 24, 11, 3, 5, 113, 381, 29, 252, 740, 76, 3, 3, 3, 17, 2], [20, 31, 197, 5, 13, 44, 39, 989, 154, 109, 2060, 3, 7, 1716, 1952, 5, 20, 585, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Sanity check of the functions in the previous cell\n",
    "#\n",
    "\n",
    "inputs = fold['train'][:2]\n",
    "print('inputs =', inputs, '/{}/'.format(len(inputs)))\n",
    "labels = make_labels(inputs)\n",
    "print('\\nlabels = ', labels, '/{}/'.format(len(labels))) \n",
    "\n",
    "MAXLEN = 50\n",
    "\n",
    "print('\\n')\n",
    "padded_inputs = pad_sequences(inputs, MAXLEN)\n",
    "print(padded_inputs)\n",
    "print('\\n')\n",
    "print(make_labels(padded_inputs)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085561cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the dataset class to hold the (padded) input id sequences referred to as 'encodings'\n",
    "# and the corresponding (padded) label sequences. Note that making labels and padding the\n",
    "# input and labels could be made therein also, having make_labels() and pad_sequences() as\n",
    "# private methods of the class.\n",
    "#\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, _encodings, _labels, _nlabels):\n",
    "        \n",
    "        assert(len(_encodings) == len(_labels))\n",
    "        \n",
    "        self.nsamples = len(_labels)\n",
    "        \n",
    "        self.encodings = _encodings # list[list[int]]: contains the padded list of token ids for each sample\n",
    "        self.labels = _labels # list[list[int]]: contains the label sequence for each sample\n",
    "        self.nlabels = _nlabels # int: number of labels in the dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns a dictionary containing the label and padded token ids for a sample\n",
    "        '''\n",
    "\n",
    "        return {'ids': torch.tensor(self.encodings[idx]), 'label': torch.tensor(self.labels[idx])}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples\n",
    "\n",
    "\n",
    "ds = dict()\n",
    "MAXLEN = 50\n",
    "nlabels = len(vocab)\n",
    "\n",
    "for x in (['train', 'valid', 'test']):\n",
    "    inputs = pad_sequences(fold[x], MAXLEN)\n",
    "    labels = make_labels(inputs)\n",
    "\n",
    "    ds[x] = LMDataset(inputs, labels, nlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6587c",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "### Toy examples of basic operations with embedding and lstm layers in pyTorch\n",
    "\n",
    "These toy examples on a non-trained models are provided to get a better understanding of how to manipulate the different layers that will constitute the actual model. In particular, we show you how to write a sequence generation loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20626a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 14913\n",
      "Number of training batches: 1865\n",
      "input shape: torch.Size([8, 50])\n",
      "torch.Size([8, 50, 200])\n",
      "outputs shape torch.Size([8, 50, 200])\n",
      "logits shape torch.Size([8, 50, 7257])\n",
      "probas shape torch.Size([8, 50, 7257])\n",
      "labels shape torch.Size([8, 50])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Understanding the model forward propagation on a toy example before digging into the actual code\n",
    "#\n",
    "\n",
    "BATCHSIZE = 8 # make small batches for illustration purposes\n",
    "\n",
    "loader = DataLoader(ds['train'], batch_size=BATCHSIZE, shuffle=True)\n",
    "print('Number of samples:', len(ds['train']))\n",
    "print(f'Number of training batches:', len(loader))\n",
    "\n",
    "#\n",
    "# Model operations\n",
    "#\n",
    "batch = next(iter(loader)) # take one batch to play with\n",
    "vocsize = len(vocab)\n",
    "dim = 200\n",
    "\n",
    "embedder = torch.nn.Embedding(vocsize, dim, padding_idx = 0) # embedding layer\n",
    "rnn = torch.nn.LSTM(dim, dim, batch_first=True) # LSTM recurrence\n",
    "mlp = torch.nn.Linear(dim, vocsize) # feed forward\n",
    "softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "inputs = batch['ids']\n",
    "print('input shape:', inputs.shape)\n",
    "\n",
    "embeddings = embedder(batch['ids'])\n",
    "print(embeddings.shape)\n",
    "\n",
    "h0 = torch.zeros(1, BATCHSIZE, dim)\n",
    "c0 = torch.zeros(1, BATCHSIZE, dim)\n",
    "\n",
    "outputs, _ = rnn(embeddings, (h0, c0)) # lstm(embeddings, hidden0)\n",
    "print('outputs shape', outputs.shape)\n",
    "\n",
    "logits = mlp(outputs)\n",
    "print('logits shape', logits.shape)\n",
    "\n",
    "probas = softmax(logits)\n",
    "print('probas shape', probas.shape)\n",
    "\n",
    "\n",
    "#\n",
    "# Loss computation\n",
    "#\n",
    "# Note the ignore_index argument of the loss function that tells not to compute the\n",
    "# loss and backpropagate gradient from the points that have that label index. In \n",
    "# other words, we don't want to account for the padding in training.\n",
    "#\n",
    "# Negative log-likelihood and cross-entropy are alike if the model output consists\n",
    "# of a probability distribution (after softmax). If the output are logits (before\n",
    "# softmax then), CrossEntropyLoss() must be used as it includes the normalization.\n",
    "#\n",
    "loss_fn = torch.nn.NLLLoss(ignore_index=vocab['PAD'])\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(ignore_index=vocab['PAD'])\n",
    "labels = batch['label']\n",
    "print('labels shape', labels.shape)\n",
    "\n",
    "# have to permute because of loss function implementation\n",
    "# see https://discuss.pytorch.org/t/loss-function-format-for-sequence-ner-pos-tagging/57548\n",
    "loss = loss_fn(probas.permute(0,2,1), labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1a8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS additional\n",
      "BOS additional set\n",
      "BOS additional set jealous\n",
      "BOS additional set jealous =\n",
      "BOS additional set jealous = horrifying\n",
      "BOS additional set jealous = horrifying present\n",
      "BOS additional set jealous = horrifying present portrays\n",
      "BOS additional set jealous = horrifying present portrays nudity\n",
      "BOS additional set jealous = horrifying present portrays nudity attracted\n",
      "BOS additional set jealous = horrifying present portrays nudity attracted joe\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generation loop with such a model\n",
    "#\n",
    "\n",
    "#\n",
    "# random initialization of the reccurence state h (and cell state c since\n",
    "# LSTMs also have cell states in addition to recurrence state)\n",
    "#\n",
    "h = torch.randn(1, dim)\n",
    "c = torch.randn(1, dim)\n",
    "\n",
    "#\n",
    "# initialize utterance to start of sentence token\n",
    "#\n",
    "ids = [vocab['BOS']]\n",
    "\n",
    "#\n",
    "# iterate up to a maximum length (here 20)\n",
    "#\n",
    "for i in range(10):\n",
    "    inputs = torch.tensor([ids[-1]]) # input to single recurrence step is the last token\n",
    "    embeddings = embedder(inputs) # get embedding of single/last token    \n",
    "    outputs, (h, c) = rnn(embeddings, (h, c)) # run one step of the LSTM recurrence\n",
    "    probas = softmax(mlp(outputs)) # get the pdf over the vocabulary\n",
    "    \n",
    "    next_id = torch.argmax(probas) # get best guess -- to get random guess, use torch.multinomial()\n",
    "    ids.append(next_id)\n",
    "    \n",
    "    if next_id == vocab['EOS']: \n",
    "        break\n",
    "    \n",
    "    print(decode_utterance(ids))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677b548",
   "metadata": {},
   "source": [
    "### Define and train the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9c485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the model as a torch.nn.Module\n",
    "#\n",
    "\n",
    "class LMRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocsize, nclasses, embed_dim = 200, dropout = None):\n",
    "        super(LMRNN, self).__init__()\n",
    "\n",
    "        self.nclasses = vocsize\n",
    "        self.vocabulary_size = vocsize\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocsize, embed_dim, padding_idx = 0)\n",
    "        if dropout != None:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "            self.rnn = torch.nn.LSTM(embed_dim, embed_dim, dropout = dropout, batch_first=True)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "            self.rnn = torch.nn.LSTM(embed_dim, embed_dim, batch_first=True) \n",
    "        self.linear = torch.nn.Linear(embed_dim, nclasses)\n",
    "        # self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "  \n",
    "    def forward(self, **kwargs):\n",
    "        x = self.embedding(kwargs['ids']) # embed input ids -- batch_size * maxlen * embed_dim\n",
    "        if self.dropout != None: # dropout embeddings\n",
    "            x = self.dropout(x)        \n",
    "        x, _ =  self.rnn(x) # run through recurrent cells -- batch_size * maxlen * embed_dim\n",
    "        x = self.linear(x) # project into posterior probabilities -- batch_size * maxlen * nclasses\n",
    "        \n",
    "        return x.permute(0, 2, 1) # because loss function wants batch_size * nclasses * maxlen\n",
    "    \n",
    "          \n",
    "lm = LMRNN(len(vocab), nclasses = ds['train'].nlabels, embed_dim = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6606c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Quick adaptation of the train_step() and eval_step() functions from previous lecture. Only\n",
    "# the latter requires changes because (i) label prediction accuracy makes little sense for\n",
    "# language models and (ii) we're dealing with sequences of predictions and not a prediction\n",
    "# for document.\n",
    "#\n",
    "# We will limit ourselves to the negative log-likelihood (the loss function used here) but \n",
    "# the real metric that should be monitored is known as perplexity (should implement that one\n",
    "# day).\n",
    "#\n",
    "\n",
    "def train_step(_model, _loader, _loss, _optim, device=\"cpu\", report=0):\n",
    "    '''\n",
    "    Generic training step.\n",
    "\n",
    "    Assumes loader returns batches where the labels are accessed with the 'label' keyword.\n",
    "    All other keywords are passed as **kwargs to the model.\n",
    "    \n",
    "    If report is set to a number, reports stats on training every 'report' batches.\n",
    "\n",
    "    :return: total_loss accumulated throughout the epoch\n",
    "    '''\n",
    "\n",
    "    _model.train(True)\n",
    "    total_loss = 0.\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(_loader):\n",
    "        _optim.zero_grad()\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        outputs = _model(**inputs)\n",
    "\n",
    "        loss = _loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        _optim.step()\n",
    "\n",
    "        if report != 0 and i % report == report - 1:\n",
    "            print('  batch {} avg. loss per batch={:.4f}'.format(i + 1, running_loss / report))\n",
    "            running_loss = 0.\n",
    "\n",
    "    _model.train(False)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def eval_step(_model, _loader, device='cpu', loss_fn=None):\n",
    "    '''\n",
    "    Evaluate the model's performance on data within loader.\n",
    "    \n",
    "    :return: \n",
    "    total_loss accumulated throughout the batches\n",
    "    '''\n",
    "    \n",
    "    _model.eval()  # disable training mode\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in _loader:\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _model(**inputs)\n",
    "\n",
    "        if loss_fn != None:\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6571b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda device\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get ready with the necessary equipment for training: parameters, optimizer, loss, etc.\n",
    "#\n",
    "\n",
    "lr = 5e-4\n",
    "nepochs = 20\n",
    "report_freq = 50\n",
    "batch_size = 32\n",
    "\n",
    "# check what device we can work on\n",
    "if torch.backends.mps.is_built(): # MPS GPU library for MacOS -- requires metal to be installed\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available(): # CUDA GPU acceleration available\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Running on {device} device')\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(lm.parameters(), lr=lr)\n",
    "celoss = torch.nn.CrossEntropyLoss(ignore_index=vocab['PAD'])\n",
    "\n",
    "# create batches within loaders\n",
    "loader = dict()\n",
    "loader['train'] = DataLoader(ds['train'], batch_size=batch_size, shuffle=True) # set to False for debugging purposes\n",
    "loader['valid'] = DataLoader(ds['valid'], batch_size=batch_size)\n",
    "loader['test'] = DataLoader(ds['test'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e37cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "  batch 50 avg. loss per batch=7.6473\n",
      "  batch 100 avg. loss per batch=6.0803\n",
      "  batch 150 avg. loss per batch=5.9348\n",
      "  batch 200 avg. loss per batch=5.7924\n",
      "  batch 250 avg. loss per batch=5.6994\n",
      "  batch 300 avg. loss per batch=5.6535\n",
      "  batch 350 avg. loss per batch=5.5811\n",
      "  batch 400 avg. loss per batch=5.5572\n",
      "  batch 450 avg. loss per batch=5.4967\n",
      "  **train** avg_loss=5.9178\n",
      "  **valid** avg_loss=5.4548\n",
      "epoch: 1\n",
      "  batch 50 avg. loss per batch=5.3937\n",
      "  batch 100 avg. loss per batch=5.3715\n",
      "  batch 150 avg. loss per batch=5.3768\n",
      "  batch 200 avg. loss per batch=5.3404\n",
      "  batch 250 avg. loss per batch=5.3209\n",
      "  batch 300 avg. loss per batch=5.2816\n",
      "  batch 350 avg. loss per batch=5.2655\n",
      "  batch 400 avg. loss per batch=5.2396\n",
      "  batch 450 avg. loss per batch=5.2301\n",
      "  **train** avg_loss=5.3077\n",
      "  **valid** avg_loss=5.2383\n",
      "epoch: 2\n",
      "  batch 50 avg. loss per batch=5.1813\n",
      "  batch 100 avg. loss per batch=5.1516\n",
      "  batch 150 avg. loss per batch=5.1298\n",
      "  batch 200 avg. loss per batch=5.1041\n",
      "  batch 250 avg. loss per batch=5.1175\n",
      "  batch 300 avg. loss per batch=5.1473\n",
      "  batch 350 avg. loss per batch=5.0926\n",
      "  batch 400 avg. loss per batch=5.0737\n",
      "  batch 450 avg. loss per batch=5.0605\n",
      "  **train** avg_loss=5.1142\n",
      "  **valid** avg_loss=5.1204\n",
      "epoch: 3\n",
      "  batch 50 avg. loss per batch=4.9934\n",
      "  batch 100 avg. loss per batch=5.0252\n",
      "  batch 150 avg. loss per batch=4.9903\n",
      "  batch 200 avg. loss per batch=4.9743\n",
      "  batch 250 avg. loss per batch=4.9956\n",
      "  batch 300 avg. loss per batch=4.9750\n",
      "  batch 350 avg. loss per batch=4.9508\n",
      "  batch 400 avg. loss per batch=4.9839\n",
      "  batch 450 avg. loss per batch=4.9505\n",
      "  **train** avg_loss=4.9779\n",
      "  **valid** avg_loss=5.0408\n",
      "epoch: 4\n",
      "  batch 50 avg. loss per batch=4.8867\n",
      "  batch 100 avg. loss per batch=4.8818\n",
      "  batch 150 avg. loss per batch=4.8952\n",
      "  batch 200 avg. loss per batch=4.8946\n",
      "  batch 250 avg. loss per batch=4.8711\n",
      "  batch 300 avg. loss per batch=4.8928\n",
      "  batch 350 avg. loss per batch=4.8634\n",
      "  batch 400 avg. loss per batch=4.8395\n",
      "  batch 450 avg. loss per batch=4.8403\n",
      "  **train** avg_loss=4.8725\n",
      "  **valid** avg_loss=4.9809\n",
      "epoch: 5\n",
      "  batch 50 avg. loss per batch=4.7921\n",
      "  batch 100 avg. loss per batch=4.7828\n",
      "  batch 150 avg. loss per batch=4.7720\n",
      "  batch 200 avg. loss per batch=4.7697\n",
      "  batch 250 avg. loss per batch=4.7911\n",
      "  batch 300 avg. loss per batch=4.7708\n",
      "  batch 350 avg. loss per batch=4.7697\n",
      "  batch 400 avg. loss per batch=4.7758\n",
      "  batch 450 avg. loss per batch=4.8164\n",
      "  **train** avg_loss=4.7822\n",
      "  **valid** avg_loss=4.9370\n",
      "epoch: 6\n",
      "  batch 50 avg. loss per batch=4.6620\n",
      "  batch 100 avg. loss per batch=4.7128\n",
      "  batch 150 avg. loss per batch=4.7009\n",
      "  batch 200 avg. loss per batch=4.7310\n",
      "  batch 250 avg. loss per batch=4.6807\n",
      "  batch 300 avg. loss per batch=4.6991\n",
      "  batch 350 avg. loss per batch=4.7122\n",
      "  batch 400 avg. loss per batch=4.6986\n",
      "  batch 450 avg. loss per batch=4.6962\n",
      "  **train** avg_loss=4.6991\n",
      "  **valid** avg_loss=4.9039\n",
      "epoch: 7\n",
      "  batch 50 avg. loss per batch=4.6176\n",
      "  batch 100 avg. loss per batch=4.6000\n",
      "  batch 150 avg. loss per batch=4.6249\n",
      "  batch 200 avg. loss per batch=4.6006\n",
      "  batch 250 avg. loss per batch=4.6175\n",
      "  batch 300 avg. loss per batch=4.6450\n",
      "  batch 350 avg. loss per batch=4.6431\n",
      "  batch 400 avg. loss per batch=4.6330\n",
      "  batch 450 avg. loss per batch=4.6174\n",
      "  **train** avg_loss=4.6229\n",
      "  **valid** avg_loss=4.8751\n",
      "epoch: 8\n",
      "  batch 50 avg. loss per batch=4.5514\n",
      "  batch 100 avg. loss per batch=4.5495\n",
      "  batch 150 avg. loss per batch=4.5526\n",
      "  batch 200 avg. loss per batch=4.5874\n",
      "  batch 250 avg. loss per batch=4.5580\n",
      "  batch 300 avg. loss per batch=4.5771\n",
      "  batch 350 avg. loss per batch=4.5364\n",
      "  batch 400 avg. loss per batch=4.5445\n",
      "  batch 450 avg. loss per batch=4.5078\n",
      "  **train** avg_loss=4.5501\n",
      "  **valid** avg_loss=4.8530\n",
      "epoch: 9\n",
      "  batch 50 avg. loss per batch=4.4616\n",
      "  batch 100 avg. loss per batch=4.4715\n",
      "  batch 150 avg. loss per batch=4.4584\n",
      "  batch 200 avg. loss per batch=4.5017\n",
      "  batch 250 avg. loss per batch=4.4746\n",
      "  batch 300 avg. loss per batch=4.5069\n",
      "  batch 350 avg. loss per batch=4.4664\n",
      "  batch 400 avg. loss per batch=4.4985\n",
      "  batch 450 avg. loss per batch=4.5063\n",
      "  **train** avg_loss=4.4847\n",
      "  **valid** avg_loss=4.8369\n",
      "epoch: 10\n",
      "  batch 50 avg. loss per batch=4.4052\n",
      "  batch 100 avg. loss per batch=4.4205\n",
      "  batch 150 avg. loss per batch=4.4232\n",
      "  batch 200 avg. loss per batch=4.4248\n",
      "  batch 250 avg. loss per batch=4.4152\n",
      "  batch 300 avg. loss per batch=4.4390\n",
      "  batch 350 avg. loss per batch=4.4296\n",
      "  batch 400 avg. loss per batch=4.4265\n",
      "  batch 450 avg. loss per batch=4.3976\n",
      "  **train** avg_loss=4.4218\n",
      "  **valid** avg_loss=4.8231\n",
      "epoch: 11\n",
      "  batch 50 avg. loss per batch=4.3468\n",
      "  batch 100 avg. loss per batch=4.3403\n",
      "  batch 150 avg. loss per batch=4.3400\n",
      "  batch 200 avg. loss per batch=4.3631\n",
      "  batch 250 avg. loss per batch=4.3749\n",
      "  batch 300 avg. loss per batch=4.3866\n",
      "  batch 350 avg. loss per batch=4.3497\n",
      "  batch 400 avg. loss per batch=4.3667\n",
      "  batch 450 avg. loss per batch=4.3647\n",
      "  **train** avg_loss=4.3591\n",
      "  **valid** avg_loss=4.8182\n",
      "epoch: 12\n",
      "  batch 50 avg. loss per batch=4.2600\n",
      "  batch 100 avg. loss per batch=4.2594\n",
      "  batch 150 avg. loss per batch=4.3008\n",
      "  batch 200 avg. loss per batch=4.3177\n",
      "  batch 250 avg. loss per batch=4.3115\n",
      "  batch 300 avg. loss per batch=4.3091\n",
      "  batch 350 avg. loss per batch=4.3212\n",
      "  batch 400 avg. loss per batch=4.3303\n",
      "  batch 450 avg. loss per batch=4.3045\n",
      "  **train** avg_loss=4.3028\n",
      "  **valid** avg_loss=4.8113\n",
      "epoch: 13\n",
      "  batch 50 avg. loss per batch=4.2309\n",
      "  batch 100 avg. loss per batch=4.2218\n",
      "  batch 150 avg. loss per batch=4.2224\n",
      "  batch 200 avg. loss per batch=4.2362\n",
      "  batch 250 avg. loss per batch=4.2504\n",
      "  batch 300 avg. loss per batch=4.2560\n",
      "  batch 350 avg. loss per batch=4.2535\n",
      "  batch 400 avg. loss per batch=4.2645\n",
      "  batch 450 avg. loss per batch=4.2729\n",
      "  **train** avg_loss=4.2456\n",
      "  **valid** avg_loss=4.8082\n",
      "epoch: 14\n",
      "  batch 50 avg. loss per batch=4.1602\n",
      "  batch 100 avg. loss per batch=4.2000\n",
      "  batch 150 avg. loss per batch=4.1780\n",
      "  batch 200 avg. loss per batch=4.1593\n",
      "  batch 250 avg. loss per batch=4.1870\n",
      "  batch 300 avg. loss per batch=4.2050\n",
      "  batch 350 avg. loss per batch=4.2206\n",
      "  batch 400 avg. loss per batch=4.1967\n",
      "  batch 450 avg. loss per batch=4.2163\n",
      "  **train** avg_loss=4.1907\n",
      "  **valid** avg_loss=4.8048\n",
      "epoch: 15\n",
      "  batch 50 avg. loss per batch=4.1123\n",
      "  batch 100 avg. loss per batch=4.1298\n",
      "  batch 150 avg. loss per batch=4.1524\n",
      "  batch 200 avg. loss per batch=4.1401\n",
      "  batch 250 avg. loss per batch=4.1287\n",
      "  batch 300 avg. loss per batch=4.1437\n",
      "  batch 350 avg. loss per batch=4.1263\n",
      "  batch 400 avg. loss per batch=4.1540\n",
      "  batch 450 avg. loss per batch=4.1507\n",
      "  **train** avg_loss=4.1402\n",
      "  **valid** avg_loss=4.8111\n",
      "epoch: 16\n",
      "  batch 50 avg. loss per batch=4.0574\n",
      "  batch 100 avg. loss per batch=4.0637\n",
      "  batch 150 avg. loss per batch=4.0598\n",
      "  batch 200 avg. loss per batch=4.1111\n",
      "  batch 250 avg. loss per batch=4.0989\n",
      "  batch 300 avg. loss per batch=4.0722\n",
      "  batch 350 avg. loss per batch=4.1179\n",
      "  batch 400 avg. loss per batch=4.1160\n",
      "  batch 450 avg. loss per batch=4.0870\n",
      "  **train** avg_loss=4.0870\n",
      "  **valid** avg_loss=4.8133\n",
      "epoch: 17\n",
      "  batch 50 avg. loss per batch=4.0185\n",
      "  batch 100 avg. loss per batch=4.0158\n",
      "  batch 150 avg. loss per batch=4.0110\n",
      "  batch 200 avg. loss per batch=4.0517\n",
      "  batch 250 avg. loss per batch=4.0548\n",
      "  batch 300 avg. loss per batch=4.0429\n",
      "  batch 350 avg. loss per batch=4.0334\n",
      "  batch 400 avg. loss per batch=4.0359\n",
      "  batch 450 avg. loss per batch=4.0577\n",
      "  **train** avg_loss=4.0365\n",
      "  **valid** avg_loss=4.8215\n",
      "epoch: 18\n",
      "  batch 50 avg. loss per batch=3.9648\n",
      "  batch 100 avg. loss per batch=3.9898\n",
      "  batch 150 avg. loss per batch=3.9455\n",
      "  batch 200 avg. loss per batch=3.9802\n",
      "  batch 250 avg. loss per batch=3.9970\n",
      "  batch 300 avg. loss per batch=4.0082\n",
      "  batch 350 avg. loss per batch=3.9986\n",
      "  batch 400 avg. loss per batch=4.0007\n",
      "  batch 450 avg. loss per batch=4.0054\n",
      "  **train** avg_loss=3.9889\n",
      "  **valid** avg_loss=4.8300\n",
      "epoch: 19\n",
      "  batch 50 avg. loss per batch=3.8959\n",
      "  batch 100 avg. loss per batch=3.8994\n",
      "  batch 150 avg. loss per batch=3.9525\n",
      "  batch 200 avg. loss per batch=3.9353\n",
      "  batch 250 avg. loss per batch=3.9486\n",
      "  batch 300 avg. loss per batch=3.9626\n",
      "  batch 350 avg. loss per batch=3.9494\n",
      "  batch 400 avg. loss per batch=3.9449\n",
      "  batch 450 avg. loss per batch=3.9560\n",
      "  **train** avg_loss=3.9402\n",
      "  **valid** avg_loss=4.8359\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# And finally run the training loop\n",
    "#\n",
    "\n",
    "\n",
    "lm.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    total_loss = train_step(lm, loader['train'], celoss, optimizer, device=device, report=report_freq)\n",
    "    \n",
    "    val_loss = eval_step(lm, loader['valid'], device=device, loss_fn=celoss)\n",
    "\n",
    "    print('  **train** avg_loss={:.4f}'.format(total_loss / len(loader['train'])))\n",
    "    print('  **valid** avg_loss={:.4f}'.format(val_loss / len(loader['valid'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "deda32be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMRNN(\n",
       "  (embedding): Embedding(7257, 200, padding_idx=0)\n",
       "  (rnn): LSTM(200, 200, batch_first=True)\n",
       "  (linear): Linear(in_features=200, out_features=7257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# This cell teaches you how to save/load a model. Because training takes a bit of time with \n",
    "# recurrent neural networks, you'd better save your model once trained so that you can load\n",
    "# it back at any point and play with generation.\n",
    "# \n",
    "# \n",
    "fn = './rnnlm-imdb-pos.pt'\n",
    "\n",
    "#torch.save(lm.state_dict(), fn)\n",
    "\n",
    "#\n",
    "# The following lines assume the instance lm of the LMRNN() class has been created and simply\n",
    "# reload the weights.\n",
    "#\n",
    "lm.load_state_dict(torch.load(fn, weights_only=True))\n",
    "lm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c548bf",
   "metadata": {},
   "source": [
    "## Play with the model to generate texts\n",
    "\n",
    "Now that you have a language model fully trained, you should be able to generate texts and/or compute\n",
    "the probability of a given input. At the very least, you should write a function that randomly generates\n",
    "a sentence given the model. If time permits or if curiosity keeps you awake, you could extend this \n",
    "generation function to be able to complete a given prompt: i.e., run tghe prompt through the RNN to \n",
    "get the RNN states before startig the generation loop.\n",
    "\n",
    "Keep in mind that the model as defined above outputs logits and not sofmax-normalized probabilities.\n",
    "\n",
    "Note that in practice, these functions are usually defined as methods within the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77249206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_generate(_model, maxlen: int = 20) -> list[int]:\n",
    "    '''\n",
    "    Random generation of a text given the model.\n",
    "    '''\n",
    "    sequence = [vocab['BOS']]\n",
    "    \n",
    "\n",
    "    h0 = torch.randn(1, dim).to(device)\n",
    "    c0 = torch.randn(1, dim).to(device)\n",
    "\n",
    "\n",
    "    for _ in range(maxlen-1):\n",
    "        # Get the embedding for the last token in the sequence\n",
    "        embed = _model.embedding(torch.tensor([sequence[-1]]).to(device))\n",
    "\n",
    "        # Pass the embedding through the RNN (LSTM in this case)\n",
    "        outputs, (h0, c0) = _model.rnn(embed, (h0, c0))  # lstm(embeddings, hidden_state)\n",
    "\n",
    "        # Pass the RNN output through the linear layer to get logits\n",
    "        logits = _model.linear(outputs.squeeze(0))\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probas = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the next token by taking the argmax of the probabilities\n",
    "        next_token = torch.argmax(probas, dim=-1).item()\n",
    "\n",
    "        # Append the predicted token to the sequence\n",
    "        sequence.append(next_token)\n",
    "\n",
    "        # Break the loop if the End of Sentence (EOS) token is generated\n",
    "        if next_token == vocab['EOS']:\n",
    "            break\n",
    "\n",
    "    return sequence\n",
    "    \n",
    "\n",
    "    \n",
    "def prompt_generate(_model, _promt: list[int], maxlen: int = 20) -> list[int]:\n",
    "    '''\n",
    "    Complete prompt with the most likely completion for the given the model.\n",
    "    '''\n",
    "    # Initialize the sequence with the given prompt\n",
    "    sequence = _promt[:]\n",
    "\n",
    "    # Initialize hidden and cell states for LSTM with random values\n",
    "    h0 = torch.randn(1, dim).to(device)\n",
    "    c0 = torch.randn(1, dim).to(device)\n",
    "\n",
    "    for _ in range(maxlen - len(_promt)):\n",
    "        # Get the embedding for the last token in the sequence\n",
    "        embed = _model.embedding(torch.tensor([sequence[-1]]).to(device))\n",
    "\n",
    "        # Pass the embedding through the RNN (LSTM in this case)\n",
    "        outputs, (h0, c0) = _model.rnn(embed, (h0, c0))  # lstm(embeddings, hidden_state)\n",
    "\n",
    "        # Pass the RNN output through the linear layer to get logits\n",
    "        logits = _model.linear(outputs.squeeze(0))\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probas = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the next token by taking the argmax of the probabilities\n",
    "        next_token = torch.argmax(probas, dim=-1).item()\n",
    "\n",
    "        # Append the predicted token to the sequence\n",
    "        sequence.append(next_token)\n",
    "\n",
    "        # Break the loop if the End of Sentence (EOS) token is generated\n",
    "        if next_token == vocab['EOS']:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be0a8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 118, 13, 21, 8, 61, 24, 6, 14, 21, 32, 3, 6, 14, 33, 10, 139, 15, 16]\n",
      "BOS i think it was a great movie . i was n't UNK . i have to say that this\n",
      "[1, 14, 46, 4946, 4502, 6852, 4948, 5062, 7103, 3508, 5929, 3887, 5278, 3536, 4653, 6085, 6065, 3961, 5556, 6174]\n",
      "BOS i like celluloid l. gesture deliciously screening ego importantly denver stadium veronica karl kudos sarandon chest mice fragile trotta\n"
     ]
    }
   ],
   "source": [
    "sequence = random_generate(lm)\n",
    "print(sequence)\n",
    "print(decode_utterance(sequence))\n",
    "\n",
    "sequence = prompt_generate(lm, [vocab['BOS'], vocab['i'], vocab['like']], maxlen=20)\n",
    "print(sequence)\n",
    "print(decode_utterance(sequence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
