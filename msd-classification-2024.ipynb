{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7adbe2",
   "metadata": {},
   "source": [
    "# Document classification notebook\n",
    "\n",
    "This notebook illustrate the lecture on document classification. See first lecture notebook or moodle for explanations on how to set up the proper environment. We will basically here make use of the same environment however augmented with pytorch.\n",
    "\n",
    "## Dataset \n",
    "\n",
    "We will make use of the SST2 dataset from GLUE, a dataset dedicated to sentiment analysis. The task at hand is the classification of short utterances stating an opinion about a movue as expressing a positive (1) or a negative (0) opinion.\n",
    "\n",
    "SST2 data was initially downloaded from https://dl.fbaipublicfiles.com/glue/data/SST-2.zip. Unfortunately doesn't have a label on the test data and there are discrepancies between train and dev, the latter having punctuation marks that the former does not have. We'll thus downsize the train set and split it into train, validation and test subsets. \n",
    "\n",
    "More info on the data at https://nlp.stanford.edu/sentiment/index.html.\n",
    "\n",
    "Note that data preparation with SST2 is minimal as utterances are short, already cleaned with no punctuations and weird signs. \n",
    "\n",
    "If you want to explore further text classification and data preparation, you can also play with the IMDb dataset that contains movie reviews that are much longer than single utterances. See https://ai.stanford.edu/~amaas/data/sentiment for details. You can get the raw data from there but I'm providing you with an easier json version to load\n",
    "\n",
    "## Methods\n",
    "\n",
    "The idea of this notebook is to illustrate a number of methods for text classification. Ideally, we should implement four: \n",
    "- an average word2vec approach with a neural network classifier\n",
    "- an average embedding approach with a neural network classifier\n",
    "- a recurrent neural network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7e76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b3ea4",
   "metadata": {},
   "source": [
    "## Loading and tokenizing the dataset\n",
    "\n",
    "Let's first load the data, clean a bit and tokenize. Last cell provides basic statistics on the corpus.\n",
    "\n",
    "Punctuation marks were already removed in SST2 and all data is uncased so tokenization remains minimal here. It basically consists in separating tokens based on spaces after getting rid of weird symbols we don't want to bother with.\n",
    "\n",
    "Note that in real life, if data hasn't been pre-processed as it is now, you'd have to invoke a real tokenizer such as the ones we made us of during the first lecture (e.g., NLTK sent_tokenize and word_tokenize, one of spaCy's pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 -- class=0, string=/hide new secretions from the parental units /\n",
      "sample 1 -- class=0, string=/contains no wit , only labored gags /\n",
      "sample 2 -- class=1, string=/that loves its characters and communicates something rather beautiful about human nature /\n",
      "sample 3 -- class=0, string=/remains utterly satisfied to remain the same throughout /\n",
      "sample 4 -- class=0, string=/on the worst revenge-of-the-nerds clichés the filmmakers could dredge up /\n",
      "sample 5 -- class=0, string=/that 's far too tragic to merit such superficial treatment /\n",
      "sample 6 -- class=1, string=/demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . /\n",
      "sample 7 -- class=1, string=/of saucy /\n",
      "sample 8 -- class=0, string=/a depressed fifteen-year-old 's suicidal poetry /\n",
      "sample 9 -- class=1, string=/are more deeply thought through than in most ` right-thinking ' films /\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's first load raw data from the CSV files. \n",
    "#\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_tsv_data(fn: str) -> list[dict]:\n",
    "    '''\n",
    "    Load data from file\n",
    "    '''\n",
    "    \n",
    "    with open(fn, 'r') as f:\n",
    "        dat = [x for x in csv.DictReader(f, delimiter=\"\\t\")] \n",
    "\n",
    "    return dat    \n",
    "    \n",
    "data = load_tsv_data('./SST-2/train.tsv')\n",
    "\n",
    "for i in range(10):\n",
    "    print('sample {} -- class={}, string=/{}/'.format(i, data[i]['label'], data[i]['sentence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 -- class=0, string=/hide new secretions from the parental units /, tokens=['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']\n",
      "sample 1 -- class=0, string=/contains no wit , only labored gags /, tokens=['contains', 'no', 'wit', 'only', 'labored', 'gags']\n",
      "sample 2 -- class=1, string=/that loves its characters and communicates something rather beautiful about human nature /, tokens=['that', 'loves', 'its', 'characters', 'and', 'communicates', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']\n",
      "sample 3 -- class=0, string=/remains utterly satisfied to remain the same throughout /, tokens=['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']\n",
      "sample 4 -- class=0, string=/on the worst revenge-of-the-nerds clichés the filmmakers could dredge up /, tokens=['on', 'the', 'worst', 'revenge-of-the-nerds', 'clichés', 'the', 'filmmakers', 'could', 'dredge', 'up']\n",
      "sample 5 -- class=0, string=/that 's far too tragic to merit such superficial treatment /, tokens=['that', \"'s\", 'far', 'too', 'tragic', 'to', 'merit', 'such', 'superficial', 'treatment']\n",
      "sample 6 -- class=1, string=/demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . /, tokens=['demonstrates', 'that', 'the', 'director', 'of', 'such', 'hollywood', 'blockbusters', 'as', 'patriot', 'games', 'can', 'still', 'turn', 'out', 'a', 'small', 'personal', 'film', 'with', 'an', 'emotional', 'wallop']\n",
      "sample 7 -- class=1, string=/of saucy /, tokens=['of', 'saucy']\n",
      "sample 8 -- class=0, string=/a depressed fifteen-year-old 's suicidal poetry /, tokens=['a', 'depressed', 'fifteen-year-old', \"'s\", 'suicidal', 'poetry']\n",
      "sample 9 -- class=1, string=/are more deeply thought through than in most ` right-thinking ' films /, tokens=['are', 'more', 'deeply', 'thought', 'through', 'than', 'in', 'most', 'right-thinking', 'films']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Basic tokenization and cleansing of the utterances to classify. \n",
    "# \n",
    "\n",
    "def clean_and_tokenize_utterance(x: dict) -> dict:\n",
    "    '''\n",
    "    Tokenize utterance with basic rules and convert labels to int. Input an entry of the dataset as\n",
    "    a dict() and returns the dictionary augmented with the list of tokens (as strings). Also converts\n",
    "    the label to an integer.\n",
    "    '''\n",
    "    unwanted =  (\"``\", \"''\", \"'\", \"`\", \"--\", \",\", \".\")\n",
    "    \n",
    "    x['tokens'] = [token for token in x['sentence'].split() if token not in unwanted]\n",
    "    x['label'] = int(x['label'])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# apply the utterance-level tokenizer to each data item\n",
    "data = list(map(clean_and_tokenize_utterance, data))\n",
    "\n",
    "for i in range(10):\n",
    "    print('sample {} -- class={}, string=/{}/, tokens={}'.format(i, data[i]['label'], data[i]['sentence'], data[i]['tokens']))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Question: What do you think of the tokenization? Is it good, bad? What could cause problems later on \n",
    "# in the process of designing an utterance classifier?\n",
    "# There are words that are common in the classes of 0 and 1. There are positive words like \"merit\" that could classify as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8683fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset        67349  29780/37569\n",
      "Number of tokens per utterance      min=1  max=48  mean=8.7  median=6  sdev=7.4\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check some basic statistics on the data\n",
    "#\n",
    "\n",
    "nsamples = len(data)\n",
    "nlabels = 2\n",
    "nlabels0 = len([x for x in data if x['label'] == 0])\n",
    "nlabels1 = len([x for x in data if x['label'] == 1])\n",
    "\n",
    "ntokens = [len(x['tokens']) for x in data]\n",
    "\n",
    "m = statistics.mean(ntokens)\n",
    "m2 = statistics.median(ntokens)\n",
    "sdev = statistics.stdev(ntokens)\n",
    "    \n",
    "print('{:35s} {}  {}/{}'.format('Number of samples in dataset', nsamples, nlabels0, nlabels1))\n",
    "print('{:35s} min={}  max={}  mean={:.1f}  median={}  sdev={:.1f}'.format('Number of tokens per utterance', min(ntokens), max(ntokens), m, m2, sdev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825d1d8",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "As we will design models that define statistics on tokens, we first have to build the vocabulary that we will be able to represent. A common choice when not using subword tokenization is to consider the most frequent tokens as the vocabulary.\n",
    "\n",
    "We'll limit ourselves to simple terms (as opposed to complex terms such as 'can opener' or 'neural network') and simply select the most frequent terms in the corpus.\n",
    "\n",
    "Note that many toolkits for NLP provide a sort of equivalent function, e.g., \n",
    "- gensim.corpora.dictionary.Dictionary -- https://radimrehurek.com/gensim/corpora/dictionary.html#\n",
    "- tf.keras.preprocessing.text.Tokenizer -- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "- torchtext.vocab -- https://pytorch.org/text/stable/vocab.html\n",
    "\n",
    "But for pedagogical purposes, we'll do it once all by ourselves ;).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1641eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in dataset = 14809\n",
      "most frequent tokens:\n",
      "   the                 27205\n",
      "   a                   21609\n",
      "   and                 19920\n",
      "   of                  17907\n",
      "   to                  12538\n",
      "   's                  8764\n",
      "   is                  8685\n",
      "   that                7759\n",
      "   in                  7495\n",
      "   it                  7078\n",
      "   as                  5088\n",
      "   with                4745\n",
      "   an                  4133\n",
      "   film                4038\n",
      "   its                 3924\n",
      "   for                 3913\n",
      "   movie               3563\n",
      "   this                3365\n",
      "   you                 2749\n",
      "   but                 2690\n",
      "\n",
      "least frequent tokens:\n",
      "   beresford           1\n",
      "   trey                1\n",
      "   findings            1\n",
      "   colonialism         1\n",
      "   foo                 1\n",
      "   yung                1\n",
      "   coburn              1\n",
      "   veggietales         1\n",
      "   methodical          1\n",
      "   jeong-hyang         1\n",
      "   theorizing          1\n",
      "   colosseum           1\n",
      "   o.                  1\n",
      "   antagonism          1\n",
      "   jerusalem           1\n",
      "   featherweight       1\n",
      "   koshashvili         1\n",
      "   symbolically        1\n",
      "   iranian-american    1\n",
      "   1979                1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check all tokens that appear in the dataset and count the number of times they appear.\n",
    "#\n",
    "\n",
    "counter = Counter()\n",
    "   \n",
    "for item in data:\n",
    "    #\n",
    "    # If we were to implement filters, e.g., on the POS tags, that's the place\n",
    "    # where it could/should be done. We'd for instance create a temporary list\n",
    "    # of relevant tokens before updating counter instead of taking all the tokens\n",
    "    # as is.\n",
    "    counter.update(item['tokens'])\n",
    "\n",
    "counter = dict(sorted(counter.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(counter))\n",
    "print('most frequent tokens:')\n",
    "for x in list(counter.keys())[:20]:\n",
    "    print(f\"   {x:18}  {counter[x]}\")\n",
    "print('\\nleast frequent tokens:')\n",
    "for x in list(counter.keys())[-20:]:\n",
    "    print(f\"   {x:18}  {counter[x]}\")\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Comment the most and least frequent tokens and think about how you could get\n",
    "# a list of tokens of interest other than by selecting the most frequent ones.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfa14a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4002\n",
      "['PAD', 'UNK', 'the', 'a', 'and', 'of', 'to', \"'s\", 'is', 'that', 'in', 'it', 'as', 'with', 'an', 'film', 'its', 'for', 'movie', 'this']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's build the vocabulary from there, selecting the most frequent terms. Note that another common\n",
    "# choice is to keep tokens that appear at least a certain number of times in the dataset.\n",
    "#\n",
    "# We will include two special tokens in the vocabulary:\n",
    "#.  - PAD: the id of the padding token required for sequence models\n",
    "#   - UNK: the id to assign to tokens that are not in the vocabulary\n",
    "#\n",
    "# The vocabulary will be dictionary mapping string to ids.\n",
    "#\n",
    "\n",
    "nterms = 4000 # number of terms to keep in the vocabulary\n",
    "\n",
    "# voc = list(counter.keys())[:nterms]\n",
    "# for x in voc[-20:]:\n",
    "#    print(f\"   {x:18}  {count[x]}\")\n",
    "\n",
    "vocab = {'PAD': 0, 'UNK': 1} # initialize with the two special tokens before updating with actual regular tokens\n",
    "offset = len(vocab)\n",
    "vocab.update({x: i+offset for i,x in enumerate(list(counter.keys())[:nterms])})\n",
    "\n",
    "print(len(vocab))\n",
    "print(list(vocab.keys())[:20])\n",
    "\n",
    "# for pretty printing later on, let's build a reverse mapping id2str from token IDs to the corresponding string\n",
    "id2str = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf51b26",
   "metadata": {},
   "source": [
    "## Encoding and splitting data to create pytorch datasets and dataloaders\n",
    "\n",
    "Last step before we can start building classifier, we have to split the dataset into train/validation/test and encode input sequences into list of integers rather than list of strings. \n",
    "\n",
    "We also have to convert each dataset into a pytorch Dataset that will be batched automatically through the Dataloader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b1e38",
   "metadata": {},
   "source": [
    "### Encoding and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18abd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'tokens': ['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units'], 'ids': [1, 87, 1, 34, 2, 1, 1], 'ids_no_unk': [87, 34, 2]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Encoding the data simply means converting tokens from string to integer ids that are more suited for\n",
    "# modeling and embedding. We will simply add a field 'ids' to each data item holding the list of token ids\n",
    "#\n",
    "\n",
    "def encode_utterance(x: dict) -> dict:\n",
    "    '''\n",
    "    Encode utterance according to the mapping provided by v. Input an entry of the dataset as\n",
    "    a dict() and returns the dictionary augmented with the list of tokens (as strings). For \n",
    "    practical reasons, we're also adding a ids_no_unk field where unknown token ids are discarded.\n",
    "    '''\n",
    "    global vocab\n",
    "    \n",
    "    unk_id = vocab['UNK']\n",
    "    \n",
    "    x['ids'] = [vocab.get(token, unk_id) for token in x['tokens']] \n",
    "    x['ids_no_unk'] = [x for x in x['ids'] if x != unk_id]\n",
    "    \n",
    "    return x\n",
    "\n",
    "data = list(map(encode_utterance, data))\n",
    "\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90e7ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  = 47144  20963/26181\n",
      "valid  = 10102  4363/5739\n",
      "test   = 10103  4454/5649\n",
      "{'sentence': 'our best actors ', 'label': 1, 'tokens': ['our', 'best', 'actors'], 'ids': [202, 74, 174], 'ids_no_unk': [202, 74, 174]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's finally split the data into three datasets\n",
    "#\n",
    "\n",
    "fold = dict()\n",
    "\n",
    "fold['train'], buf = train_test_split(data, test_size=0.3, random_state=42)\n",
    "fold['valid'], fold['test'] = train_test_split(buf, test_size=0.5, random_state=42)\n",
    "\n",
    "for s in ('train', 'valid', 'test'):\n",
    "    buf = fold[s]\n",
    "    n0 = len([x for x in buf if x['label'] == 0])\n",
    "    n1 = len([x for x in buf if x['label'] == 1])\n",
    "\n",
    "    print('{:6s} = {}  {}/{}'.format(s, len(buf), n0, n1))\n",
    "\n",
    "print(fold['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3405365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save dataset folds for later reuse\n",
    "#\n",
    "\n",
    "with open('./SST-2/sst2-tokenized-folds.json', 'w') as f:\n",
    "    json.dump(fold, f)\n",
    "    \n",
    "#\n",
    "# reload\n",
    "#\n",
    "# with open('data.NOSAVE/sst2-tokenized-folds.json', 'r') as f:\n",
    "#     fold = json.load(f)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c60bc",
   "metadata": {},
   "source": [
    "### Turn datasets into a pytorch Dataset\n",
    "\n",
    "Batching requires that all sequences have the same length, at least in a batch. \n",
    "\n",
    "In practice, this is ensured with a collator function that 0-pads the sequences on a per batch basis. To make it easier and more explicit, we will force all our sequences in the dataset to have the same length right from the start and don't bother with a collator. But we have to keep in mind this is suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85abbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c009794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the dataset class to hold the labels and the (padded) input ids referred to as 'encodings'.\n",
    "#\n",
    "# I chose to tokenize, encode and pad outside the dataset but this could all be embedded\n",
    "# in the class constructor. See, e.g.,\n",
    "# as in https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=3vWRDemOGxJD\n",
    "#\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        \n",
    "        assert(len(encodings) == len(labels))\n",
    "        \n",
    "        self.nsamples = len(labels)\n",
    "        \n",
    "        # print(f'Initializing dataset with {self.nsamples} entries')\n",
    "        \n",
    "        self.encodings = encodings # list[list[int]]: contains the padded list of token ids for each sample\n",
    "        self.labels = labels # list[int]: contains the label for each sample\n",
    "        self.nlabels = len(set(labels)) # int: number of labels in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns a dictionary containing the label and padded token ids for a sample\n",
    "        '''\n",
    "        \n",
    "        # print(f'accessing dataset item at index {idx}')\n",
    "        # print(torch.tensor(self.encodings[idx]), torch.tensor(self.labels[idx]))\n",
    "\n",
    "        return {'ids': torch.tensor(self.encodings[idx]), 'label': torch.tensor(self.labels[idx])}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#\n",
    "# Define basic padding function\n",
    "#\n",
    "def pad_utterances(x: list, maxlen: int, pad_id: int = 0) -> list:\n",
    "    '''\n",
    "    Pad all input utterances up to maxlen, truncating if need be.\n",
    "    \n",
    "    Returns a list of padded ids. \n",
    "    '''\n",
    "    \n",
    "    return [(ids + [pad_id] * (maxlen - len(ids)))[:maxlen] for ids in x]\n",
    "\n",
    "\n",
    "#\n",
    "# Function to convert the dataset as a list[dict] into a proper torch.Dataset object\n",
    "# \n",
    "def to_dataset(_data: list[dict], key: str = 'ids', maxlen: int = -1, pad_id: int = 0) -> MyDataset:\n",
    "    '''\n",
    "    Convert data as processed before into a proper pyTorch dataset with the specified tokenizer. \n",
    "    If maxlen <= 0, then we take the maximum length within the sequence.\n",
    "    '''\n",
    "\n",
    "    if maxlen <= 0:\n",
    "        maxlen = max([len(x[key]) for x in _data])\n",
    "        print('maxlen set to', maxlen)\n",
    "        \n",
    "    ids = [x[key] for x in _data]\n",
    "    labels = [x['label'] for x in _data]\n",
    "    encodings = pad_utterances(ids, maxlen)\n",
    "    \n",
    "    return MyDataset(encodings, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5b5df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  nsamples = 47144  nlabels = 2\n",
      "{'sentence': 'our best actors ', 'label': 1, 'tokens': ['our', 'best', 'actors'], 'ids': [202, 74, 174], 'ids_no_unk': [202, 74, 174]}\n",
      "   >> {'ids': tensor([202,  74, 174,   0,   0,   0,   0,   0,   0,   0]), 'label': tensor(1)}\n",
      "{'sentence': 'writing , skewed characters , and the title performance by kieran culkin ', 'label': 0, 'tokens': ['writing', 'skewed', 'characters', 'and', 'the', 'title', 'performance', 'by', 'kieran', 'culkin'], 'ids': [505, 1, 51, 4, 2, 395, 140, 25, 1, 1], 'ids_no_unk': [505, 51, 4, 2, 395, 140, 25]}\n",
      "   >> {'ids': tensor([505,  51,   4,   2, 395, 140,  25,   0,   0,   0]), 'label': tensor(0)}\n",
      "{'sentence': 'a delicate ambiguity ', 'label': 1, 'tokens': ['a', 'delicate', 'ambiguity'], 'ids': [3, 776, 1], 'ids_no_unk': [3, 776]}\n",
      "   >> {'ids': tensor([  3, 776,   0,   0,   0,   0,   0,   0,   0,   0]), 'label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "ds = dict()\n",
    "\n",
    "ds['train'] = to_dataset(fold['train'], key='ids_no_unk', maxlen=10)\n",
    "ds['valid'] = to_dataset(fold['valid'], key='ids_no_unk', maxlen=10)\n",
    "ds['test'] = to_dataset(fold['test'], key='ids_no_unk', maxlen=10)\n",
    "\n",
    "print('Training set:  nsamples =', ds['train'].nsamples, ' nlabels =', ds['train'].nlabels)\n",
    "\n",
    "for i in range(3):\n",
    "    print(fold['train'][i])\n",
    "    print('   >>', ds['train'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde0e09",
   "metadata": {},
   "source": [
    "## A basic (toy) text classification neural network\n",
    "\n",
    "Let's start text classification machinery with the basic model that we saw in the lecture that embeds the tokens, takes the average embedding and run a basic feed-forward classifier on top of it.\n",
    "\n",
    "This section covers several steps in (pytorch) model design\n",
    "1. define the model, i.e., its architecture\n",
    "2. organize the dataset into batches, achieved with a Dataloader object in pytorch\n",
    "3. define the training setup, i.e., optimizer and loss function along with a few parameters (e.g., number of epochs, learning rate)\n",
    "4. run the training loop\n",
    "5. evaluate results on the test set\n",
    "\n",
    "In pytorch, the training loop has to be written explicitly (contrary to the fit() method in scikit-learn or tensorflow). Do does the evaluation loop (contrary to evaluate()). To facilitate things, two generic functions are given, namely train_step() and eval_step(): see below for more comments. These two functions are defined here since it's the first model we are conceiving but they will be used as is for the subsequent models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6bf62",
   "metadata": {},
   "source": [
    "### Creating the model and batches\n",
    "\n",
    "Let's first create the model, batch the data via a DataLoader (this is where we're happy that all encoded sequences are of the same length) and illustrate how we can pass on data for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ecad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the model as a torch.nn.Module\n",
    "#\n",
    "\n",
    "class AvgEmbeddingNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocsize, nclasses = 2, embed_dim = 200, dropout = None):\n",
    "        super(AvgEmbeddingNN, self).__init__()\n",
    "\n",
    "        self.nclasses = nclasses\n",
    "        self.vocabulary_size = vocsize\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocsize, embed_dim, padding_idx = 0)\n",
    "        self.dropout = torch.nn.Dropout(dropout) if dropout != None else None\n",
    "        self.linear = torch.nn.Linear(embed_dim, nclasses)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        x = self.embedding(kwargs['ids']) # embed input ids -- batch_size * maxlen * embed_dim\n",
    "        if self.dropout != None: # dropout embeddings\n",
    "            x = self.dropout(x)        \n",
    "        x = torch.mean(x, dim=1) # average embeddings yielding an average tensor -- batch_size * embed_dim\n",
    "        x = self.softmax(self.linear(x)) # project into posterior probabilities -- batch_size * nclasses\n",
    "\n",
    "        return x\n",
    "        \n",
    "model1 = AvgEmbeddingNN(len(vocab), nclasses = ds['train'].nlabels, embed_dim = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d4fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 47144\n",
      "Number of training batches: 1474\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Create batched dataset with data loaders\n",
    "#\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "loader = dict()\n",
    "loader['train'] = DataLoader(ds['train'], batch_size=batch_size, shuffle=True) # set to False for debugging purposes\n",
    "loader['valid'] = DataLoader(ds['valid'], batch_size=batch_size)\n",
    "loader['test'] = DataLoader(ds['test'], batch_size=batch_size)\n",
    "\n",
    "print('Number of samples:', len(ds['train']))\n",
    "print(f'Number of training batches:', len(loader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc76a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "{'ids': tensor([[3054,    2,   42,  202,  531,  480,  447,  166, 2096,    6],\n",
      "        [ 786,    2,    5,    2,   42,    0,    0,    0,    0,    0],\n",
      "        [   3,  607, 1467,  574,  381,    0,    0,    0,    0,    0],\n",
      "        [ 440,   15,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  58,  116,   38,  546,   29,  923,   38, 3206,   38,   75],\n",
      "        [   3,  426,  521,   17,  791,   36, 3333,   48,  105,    6],\n",
      "        [ 162,    2,  175,   13, 3794,    0,    0,    0,    0,    0],\n",
      "        [   2,  153,    5,   10,    2,  783,    5, 1699,    0,    0],\n",
      "        [ 325,   14,  728, 2139,    0,    0,    0,    0,    0,    0],\n",
      "        [  66,   40,   65,   53,    0,    0,    0,    0,    0,    0],\n",
      "        [ 240,   11,  967,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   6,  240,   34, 1389,   28,    2,  930, 3729,   38,    2],\n",
      "        [   2, 2926,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2026, 2606,  221,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   8,    2,  125,  507,    2,  517,    0,    0,    0,    0],\n",
      "        [  85,    2,  286,  131,   26,   36, 2237,    4,  612,  397],\n",
      "        [  11,    7,   31,    9,   53,   73,    8,   57, 1250, 2350],\n",
      "        [ 215,  537, 3073,    8,   24,   12,   53,   12,   20,  382],\n",
      "        [   3, 2754, 2897, 1573,    0,    0,    0,    0,    0,    0],\n",
      "        [  16,  282, 3137,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  65, 1192,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   8,  150,    6,    2, 1488,   34,    2,  622,    0,    0],\n",
      "        [1638,  825,   34,    2, 1216,    5,    4,    4, 3428,    2],\n",
      "        [  30,   45,    2, 1158,    5,  302,   13,   14, 3379,  569],\n",
      "        [  19,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [2246,  214,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  11, 1315,   20,   50,  623,    2,  788,    5,    3,  124],\n",
      "        [   2,   41, 3521,    5,    2,  181,    0,    0,    0,    0],\n",
      "        [   2,   74,  688,  469,    0,    0,    0,    0,    0,    0],\n",
      "        [2746,  293,    0,    0,    0,    0,    0,    0,    0,    0]]), 'label': tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1])}\n",
      "torch.Size([32, 2])\n",
      "tensor([[0.5672, 0.4328],\n",
      "        [0.5214, 0.4786],\n",
      "        [0.4598, 0.5402],\n",
      "        [0.5332, 0.4668],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5304, 0.4696],\n",
      "        [0.4562, 0.5438],\n",
      "        [0.4736, 0.5264],\n",
      "        [0.5735, 0.4265],\n",
      "        [0.5335, 0.4665],\n",
      "        [0.5401, 0.4599],\n",
      "        [0.5556, 0.4444],\n",
      "        [0.4455, 0.5545],\n",
      "        [0.5268, 0.4732],\n",
      "        [0.5197, 0.4803],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5311, 0.4689],\n",
      "        [0.4830, 0.5170],\n",
      "        [0.5092, 0.4908],\n",
      "        [0.3955, 0.6045],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5226, 0.4774],\n",
      "        [0.4679, 0.5321],\n",
      "        [0.5351, 0.4649],\n",
      "        [0.6863, 0.3137],\n",
      "        [0.4585, 0.5415],\n",
      "        [0.4990, 0.5010],\n",
      "        [0.4603, 0.5397],\n",
      "        [0.5062, 0.4938],\n",
      "        [0.5299, 0.4701],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5089, 0.4911]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# this cell is for illustration purposes only, showing how batches cab be passed and processed to\n",
    "# the model's forward method at iunference time.\n",
    "\n",
    "batch = next(iter(loader['train']))\n",
    "print(batch['ids'].shape)\n",
    "print(batch)\n",
    "\n",
    "# Option 1: passing the ids tensor directly as a named argument\n",
    "output = model1(ids=batch['ids'])\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "# Option 2: passing an arbitrary number of named arguments, of which only ids=ids will be used\n",
    "output = model1(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d8b50",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "In pytorch, we have to explicitly write the training iterations. The code will vary little from one model to the other but still, we have to write the training loop explicitly. To simplify things, you are provided with two key functions defined in the following cells. The actual training and evaluation of the model follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2076f",
   "metadata": {},
   "source": [
    "#### Defining the basic building blocks of the training loop\n",
    "\n",
    "Two key functions are provided here:\n",
    "\n",
    "- train_step(): run one epoch given a model, a data loader, a loss function and an optimizer function; the function that should work for all models we'll be playing with\n",
    "\n",
    "- eval_step(): evaluate the model's accuracy given a model, a data loader\n",
    "\n",
    "We finally instantiate in the last cell of this section the various functions that we'll be needing for the training such as the optimizer, the loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc8cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_model, _loader, _loss, _optim, device=\"cpu\", report=0):\n",
    "    '''\n",
    "    Generic training step.\n",
    "\n",
    "    Assumes loader returns batches where the labels are accessed with the 'label' keyword.\n",
    "    All other keywords are passed as **kwargs to the model.\n",
    "    \n",
    "    If report is set to a number, reports stats on training every 'report' batches.\n",
    "\n",
    "    :return: total_loss accumulated throughout the epoch\n",
    "    '''\n",
    "\n",
    "    _model.train(True)\n",
    "    total_loss = 0.\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, batch in enumerate(_loader):\n",
    "        _optim.zero_grad()\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "        outputs = _model(**inputs)\n",
    "\n",
    "        loss = _loss(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        _optim.step()\n",
    "\n",
    "        if report != 0 and i % report == report - 1:\n",
    "            print('  batch {} avg. loss per batch={:.4f}'.format(i + 1, running_loss / report))\n",
    "            running_loss = 0.\n",
    "\n",
    "    _model.train(False)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c44cf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(_model, _loader, device='cpu', loss_fn=None):\n",
    "    '''\n",
    "    Evaluate the model's performance on data within loader.\n",
    "    \n",
    "    :return: \n",
    "    total_loss accumulated throughout the batches\n",
    "    accuracy\n",
    "    '''\n",
    "    \n",
    "    # logger.debug('running eval_acc_recprec() on %s', device)\n",
    "\n",
    "    _model.eval()  # disable training mode\n",
    "\n",
    "    posteriors = torch.empty((0, _model.nclasses)).to(device)\n",
    "    labels = torch.empty((0)).to(device)\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    for batch in _loader:\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _model(**inputs)\n",
    "\n",
    "        posteriors = torch.cat((posteriors, outputs), dim=0)\n",
    "\n",
    "        if loss_fn != None:\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "    labels = labels.cpu()\n",
    "    posteriors = posteriors.cpu()\n",
    "    predictions = torch.argmax(posteriors, dim=-1)\n",
    "    \n",
    "    return total_loss, accuracy_score(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff079d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda device\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Last but not least, we have to set the training parameters and instatiate all the objects \n",
    "# needed for training.\n",
    "#\n",
    "\n",
    "lr = 1e-4\n",
    "nepochs = 10\n",
    "report_freq = 200\n",
    "\n",
    "# check what device we can work on\n",
    "if torch.backends.mps.is_built(): # MPS GPU library for MacOS -- requires metal to be installed\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available(): # CUDA GPU acceleration available\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Running on {device} device')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=lr)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d0b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "  batch 200 avg. loss per batch=0.6984\n",
      "  batch 400 avg. loss per batch=0.6919\n",
      "  batch 600 avg. loss per batch=0.6879\n",
      "  batch 800 avg. loss per batch=0.6841\n",
      "  batch 1000 avg. loss per batch=0.6817\n",
      "  batch 1200 avg. loss per batch=0.6775\n",
      "  batch 1400 avg. loss per batch=0.6759\n",
      "  **train** avg_loss=0.6847    acuracy=60.34%\n",
      "  **valid** avg_loss=0.6732    acuracy=60.80%\n",
      "epoch: 1\n",
      "  batch 200 avg. loss per batch=0.6712\n",
      "  batch 400 avg. loss per batch=0.6690\n",
      "  batch 600 avg. loss per batch=0.6661\n",
      "  batch 800 avg. loss per batch=0.6626\n",
      "  batch 1000 avg. loss per batch=0.6578\n",
      "  batch 1200 avg. loss per batch=0.6558\n",
      "  batch 1400 avg. loss per batch=0.6554\n",
      "  **train** avg_loss=0.6623    acuracy=65.28%\n",
      "  **valid** avg_loss=0.6521    acuracy=65.39%\n",
      "epoch: 2\n",
      "  batch 200 avg. loss per batch=0.6486\n",
      "  batch 400 avg. loss per batch=0.6441\n",
      "  batch 600 avg. loss per batch=0.6427\n",
      "  batch 800 avg. loss per batch=0.6371\n",
      "  batch 1000 avg. loss per batch=0.6346\n",
      "  batch 1200 avg. loss per batch=0.6343\n",
      "  batch 1400 avg. loss per batch=0.6301\n",
      "  **train** avg_loss=0.6381    acuracy=69.45%\n",
      "  **valid** avg_loss=0.6281    acuracy=69.39%\n",
      "epoch: 3\n",
      "  batch 200 avg. loss per batch=0.6239\n",
      "  batch 400 avg. loss per batch=0.6210\n",
      "  batch 600 avg. loss per batch=0.6172\n",
      "  batch 800 avg. loss per batch=0.6103\n",
      "  batch 1000 avg. loss per batch=0.6086\n",
      "  batch 1200 avg. loss per batch=0.6040\n",
      "  batch 1400 avg. loss per batch=0.6033\n",
      "  **train** avg_loss=0.6121    acuracy=72.74%\n",
      "  **valid** avg_loss=0.6042    acuracy=72.07%\n",
      "epoch: 4\n",
      "  batch 200 avg. loss per batch=0.5949\n",
      "  batch 400 avg. loss per batch=0.5935\n",
      "  batch 600 avg. loss per batch=0.5880\n",
      "  batch 800 avg. loss per batch=0.5886\n",
      "  batch 1000 avg. loss per batch=0.5868\n",
      "  batch 1200 avg. loss per batch=0.5872\n",
      "  batch 1400 avg. loss per batch=0.5787\n",
      "  **train** avg_loss=0.5876    acuracy=75.61%\n",
      "  **valid** avg_loss=0.5829    acuracy=74.27%\n",
      "epoch: 5\n",
      "  batch 200 avg. loss per batch=0.5706\n",
      "  batch 400 avg. loss per batch=0.5678\n",
      "  batch 600 avg. loss per batch=0.5695\n",
      "  batch 800 avg. loss per batch=0.5680\n",
      "  batch 1000 avg. loss per batch=0.5656\n",
      "  batch 1200 avg. loss per batch=0.5626\n",
      "  batch 1400 avg. loss per batch=0.5630\n",
      "  **train** avg_loss=0.5662    acuracy=77.39%\n",
      "  **valid** avg_loss=0.5648    acuracy=76.01%\n",
      "epoch: 6\n",
      "  batch 200 avg. loss per batch=0.5525\n",
      "  batch 400 avg. loss per batch=0.5586\n",
      "  batch 600 avg. loss per batch=0.5510\n",
      "  batch 800 avg. loss per batch=0.5483\n",
      "  batch 1000 avg. loss per batch=0.5441\n",
      "  batch 1200 avg. loss per batch=0.5459\n",
      "  batch 1400 avg. loss per batch=0.5405\n",
      "  **train** avg_loss=0.5482    acuracy=78.84%\n",
      "  **valid** avg_loss=0.5499    acuracy=77.27%\n",
      "epoch: 7\n",
      "  batch 200 avg. loss per batch=0.5350\n",
      "  batch 400 avg. loss per batch=0.5379\n",
      "  batch 600 avg. loss per batch=0.5378\n",
      "  batch 800 avg. loss per batch=0.5363\n",
      "  batch 1000 avg. loss per batch=0.5271\n",
      "  batch 1200 avg. loss per batch=0.5336\n",
      "  batch 1400 avg. loss per batch=0.5281\n",
      "  **train** avg_loss=0.5334    acuracy=79.98%\n",
      "  **valid** avg_loss=0.5377    acuracy=78.19%\n",
      "epoch: 8\n",
      "  batch 200 avg. loss per batch=0.5248\n",
      "  batch 400 avg. loss per batch=0.5222\n",
      "  batch 600 avg. loss per batch=0.5247\n",
      "  batch 800 avg. loss per batch=0.5245\n",
      "  batch 1000 avg. loss per batch=0.5212\n",
      "  batch 1200 avg. loss per batch=0.5186\n",
      "  batch 1400 avg. loss per batch=0.5150\n",
      "  **train** avg_loss=0.5210    acuracy=80.90%\n",
      "  **valid** avg_loss=0.5276    acuracy=78.96%\n",
      "epoch: 9\n",
      "  batch 200 avg. loss per batch=0.5132\n",
      "  batch 400 avg. loss per batch=0.5144\n",
      "  batch 600 avg. loss per batch=0.5081\n",
      "  batch 800 avg. loss per batch=0.5077\n",
      "  batch 1000 avg. loss per batch=0.5116\n",
      "  batch 1200 avg. loss per batch=0.5062\n",
      "  batch 1400 avg. loss per batch=0.5117\n",
      "  **train** avg_loss=0.5108    acuracy=81.61%\n",
      "  **valid** avg_loss=0.5192    acuracy=79.59%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# At last, here we go with the main training loop, iterating over epochs\n",
    "#\n",
    "\n",
    "model1.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    total_loss = train_step(model1, loader['train'], celoss, optimizer, device=device, report=report_freq)\n",
    "    _, trn_acc = eval_step(model1, loader['train'], device=device, loss_fn=None)\n",
    "    \n",
    "    val_loss, val_acc = eval_step(model1, loader['valid'], device=device, loss_fn=celoss)\n",
    "\n",
    "    print('  **train** avg_loss={:.4f}    acuracy={:.2f}%'.format(total_loss / len(loader['train']), 100 * trn_acc))\n",
    "    print('  **valid** avg_loss={:.4f}    acuracy={:.2f}%'.format(val_loss / len(loader['valid']), 100 * val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d5b0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  **test** acuracy=79.59%\n"
     ]
    }
   ],
   "source": [
    "_, tst_acc = eval_step(model1, loader['test'], device=device, loss_fn=None)\n",
    "\n",
    "print('  **test** acuracy={:.2f}%'.format(100 * tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3098d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "#\n",
    "# Visualize the word embeddings learned by the neural network with tSNE. Do we observe the \n",
    "# distributional semantic properties?\n",
    "#\n",
    "# Revisit the code above to work on lemmas rather than tokens, possibly limiting yourself to noun,\n",
    "# verbs and adjectives which are the most relevant POS tags for polarity detection (adjectives alone \n",
    "# are already pretty good)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c1300",
   "metadata": {},
   "source": [
    "## A recurrent neural network for text classification\n",
    "\n",
    "Let's illustrate here a different approach based on recurrent neural nets to model the input sequence. The model embeds the token ids, run a recurrent LSTM function to yield the last hidden state as given by the LSTM recursion, passing this last LSTM state into a feed-forward classifier. The only difference from the previous model is in the way we convert the sequence to a fixed length descriptor for the feed-forward classification layer. \n",
    "\n",
    "We basically go throught the same steps as for the previous model except that some steps can be factorized between the two models. In particular, data preparation and batching remains the same unless (note that in the previous example, we worked in 'ids_no_unk' tokens -- might be worth to check whether unknown tokens should be kept or not).\n",
    "\n",
    "For details on the implementation of LSTM cells, see \n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8043f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a toy cell to explain how the LSTM cell works and we get out of it.\n",
    "#\n",
    "\n",
    "batch = next(iter(loader['train']))\n",
    "print(batch['ids'].shape)\n",
    "print(batch)\n",
    "\n",
    "embedder = torch.nn.Embedding(len(vocab), 5, padding_idx = 0)\n",
    "lstm = torch.nn.LSTM(5, 5, batch_first=True)  # First 5 is input embedding dimension, second 5 is hidden state dimension\n",
    "\n",
    "# hidden0 = (torch.zeros(1, 32, 5), torch.zeros(1, 32, 5))\n",
    "# print(hidden0)\n",
    "\n",
    "embeddings = embedder(batch['ids'])\n",
    "print(embeddings.shape)\n",
    "\n",
    "outputs, (hs, cs) = lstm(embeddings) # lstm(embeddings, hidden0)\n",
    "print('outputs shape', outputs.shape)\n",
    "print('hidden shape', hs.shape) # hs is D * num_layers x batch_size x embed_dim\n",
    "\n",
    "print(hs[0][0]) # first sample last hidden state\n",
    "print(outputs[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56333cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the model as a torch.nn.Module\n",
    "#\n",
    "\n",
    "class LSTMNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocsize, nclasses = 2, embed_dim = 200, dropout = None):\n",
    "        super(LSTMNN, self).__init__()\n",
    "\n",
    "        self.nclasses = nclasses\n",
    "        self.vocabulary_size = vocsize\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocsize, embed_dim, padding_idx = 0)\n",
    "        if dropout != None:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "            self.rnn = torch.nn.LSTM(embed_dim, embed_dim, dropout = dropout, batch_first=True)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "            self.rnn = torch.nn.LSTM(embed_dim, embed_dim, batch_first=True) \n",
    "        self.linear = torch.nn.Linear(embed_dim, nclasses)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        x = self.embedding(kwargs['ids']) # embed input ids -- batch_size * maxlen * embed_dim\n",
    "        if self.dropout != None: # dropout embeddings\n",
    "            x = self.dropout(x)        \n",
    "        _, (x, _) =  self.rnn(x) # run through recurrent cells -- batch_size * embed_dim\n",
    "        x = self.softmax(self.linear(x[0])) # project into posterior probabilities -- batch_size * nclasses\n",
    "\n",
    "        return x\n",
    "        \n",
    "model2 = LSTMNN(len(vocab), nclasses = ds['train'].nlabels, embed_dim = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210dbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Redefining here the training parameters and objects needed for training for practical reasons. They are \n",
    "# the same as for the previous model except for the optimizer but is't more convenient to have them here \n",
    "# if we want to tweak things a bit.\n",
    "#\n",
    "\n",
    "lr = 1e-4\n",
    "nepochs = 10\n",
    "report_freq = 200\n",
    "\n",
    "# check what device we can work on\n",
    "if torch.backends.mps.is_built(): # MPS GPU library for MacOS -- requires metal to be installed\n",
    "    device = \"mps\"\n",
    "    torch.mps.empty_cache()\n",
    "elif torch.cuda.is_available(): # CUDA GPU acceleration available\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Running on {device} device')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=lr)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eea8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training loop, using the two generic functions we built before\n",
    "#\n",
    "\n",
    "model2.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    total_loss = train_step(model2, loader['train'], celoss, optimizer, device=device, report=report_freq)\n",
    "    _, trn_acc = eval_step(model2, loader['train'], device=device, loss_fn=None)\n",
    "    \n",
    "    val_loss, val_acc = eval_step(model2, loader['valid'], device=device, loss_fn=celoss)\n",
    "\n",
    "    print('  **train** avg_loss={:.4f}    acuracy={:.2f}%'.format(total_loss / len(loader['train']), 100 * trn_acc))\n",
    "    print('  **valid** avg_loss={:.4f}    acuracy={:.2f}%'.format(val_loss / len(loader['valid']), 100 * val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tst_acc = eval_step(model2, loader['test'], device=device, loss_fn=None)\n",
    "\n",
    "print('  **test** acuracy={:.2f}%'.format(100 * tst_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dc8d0",
   "metadata": {},
   "source": [
    "## Going further with the notebook\n",
    "\n",
    "These two examples are pretty straightforward models operating on fairly basic representations of the data. They are mostly intended to illustrate data preparation, model design and pytorch implementation of NLP models and recurrent neural nets. \n",
    "\n",
    "Now that you are kind of acquainted with these mechanisms, you can adapt these scripts to make things better. In particular, the following points deserve attention and I strongly invite you to consider their implementation to fully get familiar with an NLP pipeline for document classification.\n",
    "\n",
    "1. The choice of vocabulary is rather poor here and deserves significant improvement. For polarity detection, there's no need to work with \"words\" and lemmas are sufficient. Also for the approach based on average embeddings, function words (determiners, auxiliary, etc,) are not necessary and can be safely removed. You should adapt this first method so that the vocabulary is built from lemmatized input keeping only nouns, verbs and adjectives in the vocabulary. To do so, you need to run POS tagging and lemmatization (see first notebook on how to do that with NLTK for instance), adapt the vocabulary and hence the subsequent steps (preparation and encoding of the data).\n",
    "\n",
    "2. With a better choice of vocabulary, there should be much less UNK tokens, which should help the RNN approach. You are thus invited to test and compare the average word embedding and RNN approaches taking into account the UNK tokens which were discarded in the initial implementation (using ids_no_unk as key when converting the data into a pytorch Dataset). When doing so, you might want to think about whether lemmatization and/or POS tag filtering should be taken into account or not.\n",
    "\n",
    "Also, if you might want to compare with basic statistical methods based on symbolic word representations rather than embedding, for instance implementing a vector space model with tf-idf weights along with a k-nn classifier or using a polarity semantic network like sentiwordnet.\n",
    "\n",
    "There are many other things to do and we will revisit this task later with large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321aa90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
