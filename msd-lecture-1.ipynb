{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940b3679",
   "metadata": {},
   "source": [
    "## Installation instructions\n",
    "\n",
    "Hands-on practice will rely on a (fairly large) number of python packages for NLP and deep learning. If you are running on Google colaboratory, most of these packages are installed in the default environment and there's not much to do. If you want to run on your machine, you will have to install a number of things following the instructions below. Should be fairly straightforward but requires a bit of disk space.\n",
    "\n",
    "Open a shell window and follow the commands below (not sure exactly how to do that on Windows).\n",
    "\n",
    "### Step 1. Set up a virtual environment (optional but recommanded)\n",
    "\n",
    "> export LANG=en_US.UTF-8\n",
    "> python -m venv ./nlp.env\n",
    "> source ./nlp.env.bin.activate\n",
    "\n",
    "If you want to deactivate the virtual environment at some point, simply type deactivate.\n",
    "\n",
    "\n",
    "### Step 2. Add your virtual environment as a kernel in jupyter\n",
    "\n",
    "> pip install ipykernel\n",
    "> python -m ipykernel install --user --name=NLP\n",
    "\n",
    "You can now run your jupyter notebook where you'll see a NLP kernel. But it's empy as of now so we need to install a bunch of things first. \n",
    "\n",
    "### Step 3. Install the necessary packages\n",
    "\n",
    "From your favorite shell, virtual environment activated, you can add the followng package (might take some time):\n",
    "\n",
    "> pip install ntlk\n",
    "> pip install spacy\n",
    "> pip install transformers\n",
    "> pip install scikit-learn\n",
    "> pip install matplotlib\n",
    "\n",
    "You need to install spaCy's processing pipeline resources also. This also holds for colaboratory where you will have to run the following command in the first cell and reload the kernel.\n",
    "\n",
    "> python -m spacy download en_core_web_md\n",
    "\n",
    "You also need to install a number of NLTK resources that we will use. No need to restart the kernel.\n",
    "\n",
    "> python -m nltk.downloader wordnet\n",
    "> python -m nltk.downloader sentiwordnet\n",
    "> python -m nltk.downloader averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991ca6a",
   "metadata": {},
   "source": [
    "## Play with tokenization with different toolkits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bac564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kartik/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with NLTK, one of the most popular toolkit for NLP. \n",
    "# See https://www.nltk.org/ for details.\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c589f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '$', '2', 'example', 'of', 'a', 'sentence', '.', 'And', 'a', '2nd', 'sentence', '.']\n",
      "[['A', '$', '2', 'example', 'of', 'a', 'sentence', '.'], ['And', 'a', '2nd', 'sentence', '.']]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's play a bit with the notion of tokenization\n",
    "#\n",
    "\n",
    "# a basic tokenization of a text\n",
    "s='A $2 example\\nof a sentence. And a 2nd sentence.'\n",
    "print([token for token in word_tokenize(s)])\n",
    "\n",
    "# Now combining sentence tokenization with word tokenization\n",
    "print([word_tokenize(x) for x in sent_tokenize(s)])\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# You're turn to play a bit and try to fool the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf4fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kartik/miniforge3/envs/NLPenv/lib/python3.10/site-packages/spacy/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Let's try with spaCy now, another popular toolkit that comes with full processing\n",
    "# pipelines that do a lot of things. Tokenization is one of the thing spaCy does but\n",
    "# pipelines do a lot more than that. We'll get back to this later on, only looking \n",
    "# into the tokenization step as of now. See https://spacy.io/ for details.\n",
    "\n",
    "# Load spaCy's English NLP pipeline\n",
    "import spacy\n",
    "print(spacy.__file__)\n",
    "\n",
    "process = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's play again with the tokenizer embedded in the processing pipeline that spaCy implements (more details later)\n",
    "#\n",
    "\n",
    "print([token for token in process(s)])\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# You're turn to play a bit and see how it behaves on the examples you played with previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f94cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b99baaf65c4e76b050e862c1ddeda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a7c410c36b4c998865b539debff414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdff348d42a54b73b8c1c53962a5251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32402986d05140879c80de464df4e122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try a third one that is currently being used with deep transformer models.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dcdc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '$', '2', 'example', 'of', 'a', 'sentence', '.', 'And', 'a', '2nd', 'sentence', '.']\n",
      "['But', 'transform', '##ers', 'require', 'a', 'limited', 'number', 'of', 'token', '##s', 'to', 'be', 'efficient', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"But transformers require a limited number of tokens to be efficient.\")\n",
    "print(tokens)\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "#\n",
    "# What do you observe on the tokenization of the last sentence? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0312de",
   "metadata": {},
   "source": [
    "### Lexical analysis: mophology and morphosyntax\n",
    "\n",
    "In the following cells, we will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f71b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kartik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kartik/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NLTK has a number of tools to perform stemming, lemmatization and POS tagging to play with\n",
    "#\n",
    "# We start by importing the tools we want to play with in the next cell\n",
    "# \n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e7bd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word         stem         lemma     \n",
      "----------------------------------\n",
      "candy        candi        candy     \n",
      "candies      candi        candy     \n",
      "loves        love         love      \n",
      "loved        love         loved     \n",
      "lovely       love         lovely    \n",
      "argued       argu         argued    \n",
      "arguing      argu         arguing   \n",
      "\n",
      "lemma of the verb 'argued': argue\n",
      "lemma of the verb 'arguing': argue\n",
      "\n",
      "Sentence: The cat sat on the mat and had a couple of fishes for dinner.\n",
      "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'had', 'a', 'couple', 'of', 'fishes', 'for', 'dinner', '.']\n",
      "POS Tag [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('and', 'CC'), ('had', 'VBD'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('fishes', 'NNS'), ('for', 'IN'), ('dinner', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Examples of a basic stemmer and lemmatizer that takes isolated wordforms and\n",
    "# convert them to the corresponding stem/lemma.\n",
    "#\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"{:10}   {:10}   {:10}\".format('word', 'stem', 'lemma'))\n",
    "print(\"----------------------------------\")\n",
    "for w in ('candy', 'candies', 'loves', 'loved', 'lovely', 'argued', 'arguing'):\n",
    "    print(\"{:10}   {:10}   {:10}\".format(w, stemmer.stem(w), lemmatizer.lemmatize(w)))\n",
    "\n",
    "# Now, what if we pass on a POS information to the lemmatizer?\n",
    "print(\"\\nlemma of the verb 'argued':\", lemmatizer.lemmatize('argued', pos=wordnet.VERB))\n",
    "print(\"lemma of the verb 'arguing':\", lemmatizer.lemmatize('arguing', pos=wordnet.VERB))\n",
    "\n",
    "# Examples of a POS tagger\n",
    "\n",
    "# POS tagging\n",
    "s = 'The cat sat on the mat and had a couple of fishes for dinner.'\n",
    "tokens = word_tokenize(s)\n",
    "tags = pos_tag(tokens)\n",
    "print('\\nSentence:', s)\n",
    "print('Tokens:', tokens)\n",
    "print('POS Tag', tags) # if we want only tags: [x[1] for x in tags]\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Your turn to play a bit with these three steps and try to fool them. \n",
    "# What does POS tagging do with semantically ambiguous sentences?\n",
    "\n",
    "\n",
    "# GOING FURTHER\n",
    "#\n",
    "# One can combine NLTK's POS tagger and WordNetLemmatizer to create a function that converts\n",
    "# an input sentence to a list of tokens with their corresponding POS tags and lemmas. However\n",
    "# not totally straightforward.\n",
    "#\n",
    "# See https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5321a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The      DET    DT   the\n",
      "cat      NOUN   NN   cat\n",
      "sat      VERB   VBD  sit\n",
      "on       ADP    IN   on\n",
      "the      DET    DT   the\n",
      "mat      NOUN   NN   mat\n",
      "and      CCONJ  CC   and\n",
      "had      VERB   VBD  have\n",
      "a        DET    DT   a\n",
      "couple   NOUN   NN   couple\n",
      "of       ADP    IN   of\n",
      "fishes   NOUN   NNS  fish\n",
      "for      ADP    IN   for\n",
      "dinner   NOUN   NN   dinner\n",
      ".        PUNCT  .    .\n",
      "\n",
      "Sentence: The cat sat on the mat.\n",
      "The      DET    DT   the\n",
      "cat      NOUN   NN   cat\n",
      "sat      VERB   VBD  sit\n",
      "on       ADP    IN   on\n",
      "the      DET    DT   the\n",
      "mat      NOUN   NN   mat\n",
      ".        PUNCT  .    .\n",
      "\n",
      "Sentence: He had a couple of fishes for dinner.\n",
      "He       PRON   PRP  he\n",
      "had      VERB   VBD  have\n",
      "a        DET    DT   a\n",
      "couple   NOUN   NN   couple\n",
      "of       ADP    IN   of\n",
      "fishes   NOUN   NNS  fish\n",
      "for      ADP    IN   for\n",
      "dinner   NOUN   NN   dinner\n",
      ".        PUNCT  .    .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We can do it all at once with spaCy's pipeline that does tokenization and \n",
    "# POS tagging and lemmatization (and a few other things)\n",
    "#\n",
    "\n",
    "res = process(s)            # -> apply pipeline on text\n",
    "\n",
    "for token in res:           # -> iterate over tokens in doc\n",
    "  print('{:8} {:6} {:4} {}'.format(token.text, token.pos_, token.tag_, token.lemma_))\n",
    "\n",
    "#\n",
    "# You can also do it with long texts and multiple sentences\n",
    "#\n",
    "s = 'The cat sat on the mat. He had a couple of fishes for dinner.'\n",
    "res = process(s)\n",
    "\n",
    "for sent in res.sents:\n",
    "    print('\\nSentence:', sent)\n",
    "    for token in sent:\n",
    "        print('{:8} {:6} {:4} {}'.format(token.text, token.pos_, token.tag_, token.lemma_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45484e8e",
   "metadata": {},
   "source": [
    "### Syntaxtic analysis and dependency trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfec17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process2 = spacy.load('fr_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05c84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le       DET    le       det   \n",
      "chat     NOUN   chat     nsubj \n",
      "boit     VERB   boire    ROOT  \n",
      "le       DET    le       det   \n",
      "lait     NOUN   lait     obj   \n",
      ".        PUNCT  .        punct \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"ecdcdb0a5e83456ba004820b2c2c41f4-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Le</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">chat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">boit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">le</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">lait.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecdcdb0a5e83456ba004820b2c2c41f4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xr = process2('Le chat boit le lait.')\n",
    "\n",
    "for token in xr:\n",
    "    print('{:8} {:6} {:8} {:6}'.format(token.text, token.pos_, token.lemma_, token.dep_))\n",
    "   \n",
    "\n",
    "spacy.displacy.render(xr, style=\"dep\", jupyter=True)  \n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Your turn to play a bit with syntactic analysis and enter new sentences, possibly complex ones. The example \n",
    "# here is in French but you can obviously do the same with the English pipeline (replace process2 by process).\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dad773",
   "metadata": {},
   "source": [
    "### Word representations and lexical semantics\n",
    "\n",
    "We will play with the two main ways of representing lexical semantics: \n",
    "- word nets, encoding word senses and enconding lexical relations such as homonymy, synonymy, hyperonymy, hyponymy, etc. \n",
    "- distributional semantics as encoded by word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf2de",
   "metadata": {},
   "source": [
    "#### Playing with word net\n",
    "\n",
    "We we look at what's encoded in wordnet and play a bit with the senses of the token 'rat'. You can get more\n",
    "information on word net from https://wordnet.princeton.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41597690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e5f3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senses of the token 'rat' are:\n",
      "   Synset('rat.n.01')        any of various long-tailed rodents similar to but larger than a mouse\n",
      "   Synset('scab.n.01')       someone who works (or provides workers) during a strike\n",
      "   Synset('rotter.n.01')     a person who is deemed to be despicable or contemptible\n",
      "   Synset('informer.n.01')   one who reveals confidential information in return for money\n",
      "   Synset('rat.n.05')        a pad (usually made of hair) worn as part of a woman's coiffure\n",
      "   Synset('rat.v.01')        desert one's party or group of friends, for example, for one's personal advantage\n",
      "   Synset('rat.v.02')        employ scabs or strike breakers in\n",
      "   Synset('fink.v.01')       take the place of work of someone on strike\n",
      "   Synset('rat.v.04')        give (hair) the appearance of being fuller by using a rat\n",
      "   Synset('rat.v.05')        catch rats, especially with dogs\n",
      "   Synset('denounce.v.04')   give away information about somebody\n",
      "\n",
      "Senses of the noun 'rat' are:\n",
      "   Synset('rat.n.01')        any of various long-tailed rodents similar to but larger than a mouse\n",
      "   Synset('scab.n.01')       someone who works (or provides workers) during a strike\n",
      "   Synset('rotter.n.01')     a person who is deemed to be despicable or contemptible\n",
      "   Synset('informer.n.01')   one who reveals confidential information in return for money\n",
      "   Synset('rat.n.05')        a pad (usually made of hair) worn as part of a woman's coiffure\n",
      "\n",
      "Lemmas referring to 'rat.n.01': ['rat']\n",
      "Lemmas referring to 'dog.n.01': ['dog', 'domestic_dog', 'Canis_familiaris']\n",
      "\n",
      "Hypernyms of 'rat.n.01':\n",
      "   Synset('rodent.n.01')     relatively small placental mammals having a single pair of constantly growing incisor teeth specialized for gnawing\n",
      "Hyponyms of 'rat.n.01':\n",
      "   Synset('bandicoot_rat.n.01') burrowing scaly-tailed rat of India and Ceylon\n",
      "   Synset('black_rat.n.01')  common household pest originally from Asia that has spread worldwide\n",
      "   Synset('brown_rat.n.01')  common domestic rat; serious pest worldwide\n",
      "   Synset('jerboa_rat.n.01') large Australian rat with hind legs adapted for leaping\n",
      "   Synset('pocket_rat.n.01') any of various rodents with cheek pouches\n",
      "   Synset('rice_rat.n.01')   hardy agile rat of grassy marshes of Mexico and the southeastern United States\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We start by playing with the senses associated to a token (under the form of sets of synonyms (ala synsets),\n",
    "# then play with one of the senses to see what associated information we can get from the word sense. \n",
    "#\n",
    "\n",
    "print(\"Senses of the token 'rat' are:\")\n",
    "for s in wn.synsets('rat'):\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "\n",
    "print(\"\\nSenses of the noun 'rat' are:\")\n",
    "for s in wn.synsets('rat', pos=wn.NOUN):\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "# now let's focus on the rat as an animal and see what are the relations\n",
    "name = 'rat.n.01'\n",
    "synset = wn.synset(name)\n",
    "\n",
    "# we can list lemmas \n",
    "print(\"\\nLemmas referring to '{}':\".format(name), [str(lemma.name()) for lemma in synset.lemmas()])\n",
    "print(\"Lemmas referring to 'dog.n.01':\", [str(lemma.name()) for lemma in wn.synset('dog.n.01').lemmas()])\n",
    "\n",
    "# we can get lexical relations such as \n",
    "print(\"\\nHypernyms of '{}':\".format(name))\n",
    "for s in synset.hypernyms():\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "print(\"Hyponyms of '{}':\".format(name))\n",
    "for s in synset.hyponyms():\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Try another token and/or synset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6202d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 'rat.n.01' and 'man.n.01' is 0.55 with lowest common ancestor(s):\n",
      "   Synset('organism.n.01')   a living thing that has (or can develop) the ability to act or function independently\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We can use the graph of hypernyms/hyponyms (and a few other ones) to compare two senses\n",
    "#\n",
    "\n",
    "name1 = 'rat.n.01'\n",
    "synset1 = wn.synset(name1)\n",
    "\n",
    "name2 = 'man.n.01'\n",
    "synset2 = wn.synset(name2)\n",
    "\n",
    "lch = synset1.lowest_common_hypernyms(synset2)\n",
    "sim = synset1.wup_similarity(synset2)\n",
    "\n",
    "print(\"Distance between '{}' and '{}' is {:.2f} with lowest common ancestor(s):\".format(name1, name2, sim))\n",
    "for s in lch:\n",
    "    print(\"   {:25} {}\".format(str(s), s.definition()))\n",
    "\n",
    "    \n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Try a few other pairs, e.g., searching for senses closer to either 'rat.n.01' or 'man.n.01'. What\n",
    "# happens if we take a hyperonym/hyponym? Another sense of the token 'rat'?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d26845",
   "metadata": {},
   "source": [
    "#### Playing with word embeddings\n",
    "\n",
    "Word embedding is a form of distributional semantics, in particular for models that were designed with an \n",
    "explicit distributional hypothesis such as word2vec, glove, fasttext. We will use the word embeddings that are encoded within spaCy's pipeline. \n",
    "\n",
    "spaCy's pipeline comes with a \"vocabulary\", .e.g, the list of tokens that can be processed. We can access to the tokens of the vocabulary and to embedded vectors for the tokens. For the English and French pipelines, these are fastext vectors from Explosion: cf. https://github.com/explosion/spacy-vectors-builder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac12d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 300\n",
      "distance between tokens 'apple' and 'orange' is 0.590\n",
      "distance between tokens 'apple' and 'car' is 0.175\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's start by playing a bit with a few vectors, see what dimension they have and how they can \n",
    "# be used to compute semantic similarity between two tokens.\n",
    "\n",
    "# get access to the pipeline's vocabulary\n",
    "vocab = process.vocab \n",
    "\n",
    "# get three tokens from the vocabulary\n",
    "token1 = vocab['apple']\n",
    "token2 = vocab['orange']\n",
    "token3 = vocab['car']\n",
    "\n",
    "# check that a token has an embedded representation and the dimension of the embedded space\n",
    "if token1.has_vector == True:\n",
    "    print('Embedding size is', len(token1.vector))\n",
    "dim = len(token1.vector)\n",
    "\n",
    "print(\"distance between tokens 'apple' and 'orange' is {:.3f}\".format(token1.similarity(token2)))\n",
    "print(\"distance between tokens 'apple' and 'car' is {:.3f}\".format(token1.similarity(token3)))\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# You can verify that the distance between the tokens is indeed the cosine similarity between the \n",
    "# two corresponding vectors.\n",
    "#\n",
    "# Tips: in numpy (as np), np.norm() is the norm of a vector, np.linalg.dot(v1,v2) provides the dot\n",
    "# product between the two vectors. \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d3a3a",
   "metadata": {},
   "source": [
    "We will now visualize a scatter plot of the vectors for the words that are in simlex. Simlex lists pairs of words along with human judgement of how semantically related (not exactly the same as similar) they are on a scale from 0 (least similar) to 10 (most similar):\n",
    "\n",
    "love sex 6.77 \n",
    "tiger cat 7.35\n",
    "tiger tiger 10\n",
    "book paper 7.46\n",
    "computer keyboard 7.62\n",
    "computer internet 7.58\n",
    "\n",
    "We will load the simlex file, list the words/tokens therein and do a scatter plot of the word embeddings with t-SNE using scitkit learn TSNE() module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5172ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c800479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 153 pairs\n",
      "simlex contains 173 different words/tokens\n",
      "Shape of the projection Y: (173, 2)\n"
     ]
    }
   ],
   "source": [
    "# !wget https://people.irisa.fr/Guillaume.Gravier/teaching/ENSAI/data/simlex-1.csv\n",
    "\n",
    "#\n",
    "# Load simlex data from csv file\n",
    "#\n",
    "import csv\n",
    "\n",
    "simlex = []\n",
    "with open('simlex-1.csv', 'r') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "      simlex.append([row['Word 1'], row['Word 2'], row['Human (mean)']])\n",
    "print('loaded', len(simlex), 'pairs')\n",
    "\n",
    "wordlist = list(set([x[0] for x in simlex] + [x[1] for x in simlex]))\n",
    "nwords = len(wordlist)\n",
    "print('simlex contains', nwords, 'different words/tokens')\n",
    "\n",
    "#\n",
    "# scikit learn TSNE() takes as input either the list of vectors in a nwordsxdim matrix,\n",
    "# or the list of distances (not similarity) between individual points as a nwordsxnwords\n",
    "# symetric matrix. In the first case, one must provide the appropriate distance function\n",
    "# between vectors. We will rather go with the second option taking advantage of the \n",
    "# similarity() function of spaCy. Last, what we need is a matrix with pairwise distances\n",
    "# between words, not similarities! In other words d(x,x)=0, not 1. Cosine similarities are \n",
    "# between -1 (opposite directions) and 1 (same direction), the last case corresponding to\n",
    "# a null distance. We thus defined d(x,y) = 1 - cos(x,y). \n",
    "#\n",
    "X = np.empty((nwords, nwords), dtype='float32')\n",
    "for i in range(nwords):\n",
    "  X[i,:] = [1 - vocab[wordlist[i]].similarity(vocab[x]) for x in wordlist]\n",
    "\n",
    "# dirty hack because some  cosine similarities are slightly below -1 (inperfect arithmetic in computers)\n",
    "c = np.where(X < 0)\n",
    "for i in range(len(c[0])):\n",
    "  X[c[0][i], c[1][i]] = 0.0\n",
    "\n",
    "# run t-SNE 2D projection\n",
    "Y = TSNE(n_components=2, metric='precomputed', init='random', random_state=0, method='exact').fit_transform(X)\n",
    "print('Shape of the projection Y:', Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49654207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: visualize the scatter plot of points in Y, printing the corresponding words. This can be done with\n",
    "# plt.annotate('word', xy=(x_coord,y_cord)) to place the string 'word' at (x_coord,y_cord)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c59bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
